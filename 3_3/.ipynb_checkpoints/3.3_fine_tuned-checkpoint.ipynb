{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validataion data are all needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hypo_list, prem_list, target_list, max_sentence_length):\n",
    "        \"\"\"\n",
    "        @param hypo_list: list of hypo tokens\n",
    "        @param prem_list: list of prem tokens\n",
    "        @param target_list: list of newsgroup targets\n",
    "        @param max_sentence_length: fixed length of all sentence\n",
    "\n",
    "        \"\"\"\n",
    "        self.hypo_list = hypo_list\n",
    "        self.prem_list = prem_list\n",
    "        self.target_list = target_list\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        assert (len(self.hypo_list) == len(self.target_list))\n",
    "        assert (len(self.prem_list) == len(self.target_list))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hypo_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "            Triggered when you call dataset[i]\n",
    "            \"\"\"\n",
    "\n",
    "        token_hypo_idx = self.hypo_list[key][:self.max_sentence_length]\n",
    "        token_prem_idx = self.prem_list[key][:self.max_sentence_length]\n",
    "        label = self.target_list[key]\n",
    "        return [token_hypo_idx, len(token_hypo_idx), token_prem_idx, len(token_prem_idx), label]\n",
    "\n",
    "\n",
    "class NeuralNetworkPytorch(nn.Module):\n",
    "    \"\"\"\n",
    "    NeuralNetwork classification model\n",
    "    Model would change according to interaction_type\n",
    "\n",
    "    1st hidden layer: 90 neurons\n",
    "    2nd hidden layer: 90 neurons\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, n_out, interaction_type):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary.\n",
    "        @param emb_dim: size of the word embedding\n",
    "        @param n_out: size of the class.\n",
    "        \"\"\"\n",
    "        super(NeuralNetworkPytorch, self).__init__()\n",
    "\n",
    "        # 1. Embedding\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        # 2. an affine operation: y=Wx+b\n",
    "        # double embedding dimension if we concat hypo's and prem's embedding\n",
    "        if interaction_type == 'concat':\n",
    "            emb_dim = 2 * emb_dim\n",
    "        self.hidden_1= nn.Linear(emb_dim,90)\n",
    "        self.hidden_2=nn.Linear(90, 90)\n",
    "        self.output = nn.Linear(90, n_out)\n",
    "\n",
    "    def forward(self, data_hypo, length_hypo, data_prem, length_prem, interaction_type):\n",
    "        \"\"\"\n",
    "            @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a\n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "            @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            @param data_prem: matrix of size (batch_size, max_sentence_length).\n",
    "            @param length_hypo: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "                length of each sentences in the data_prem.\n",
    "            @param interaction_type: [sum. hadamart, concat]\n",
    "            \"\"\"\n",
    "        # word embedding\n",
    "        out_hypo = self.embed(data_hypo)\n",
    "        out_prem = self.embed(data_prem)\n",
    "        # combine to sentence\n",
    "        out_prem = torch.sum(out_prem, dim=1)\n",
    "        out_hypo = torch.sum(out_hypo, dim=1)\n",
    "        out_prem /= length_prem.view(length_prem.size()[0], 1).expand_as(out_prem).float()\n",
    "        out_hypo /= length_hypo.view(length_hypo.size()[0], 1).expand_as(out_hypo).float()\n",
    "\n",
    "        # interaction\n",
    "        # 1. sum\n",
    "        # 2. Hadamard product\n",
    "        # 3. concat (This will change embedding dimension, 2 times as many as before)\n",
    "        if interaction_type == 'concat':\n",
    "            out = torch.cat((out_hypo,out_prem),1)\n",
    "        if interaction_type == 'sum':\n",
    "            out = torch.add(out_hypo, out_prem)\n",
    "        if interaction_type == 'hadamard':\n",
    "            out = out_hypo * out_prem\n",
    "\n",
    "        out = self.hidden_1(out.float())\n",
    "        out = F.relu(out)\n",
    "        out = self.hidden_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    hypo_list = []\n",
    "    len_hypo_list = []\n",
    "    prem_list = []\n",
    "    len_prem_list = []\n",
    "    label_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch)\n",
    "    #batch[0][0] = batch[0][0][:max_sentence_length_prem]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        len_hypo_list.append(datum[1])\n",
    "        len_prem_list.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        # hypo\n",
    "        padded_vec = np.pad(np.array(datum[0]), pad_width=((0,max_sentence_length-datum[1])), mode=\"constant\", constant_values=0)\n",
    "        hypo_list.append(padded_vec)\n",
    "        # prem\n",
    "        padded_vec = np.pad(np.array(datum[2]), pad_width=((0,max_sentence_length-datum[3])), mode=\"constant\", constant_values=0)\n",
    "        prem_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(hypo_list)), torch.LongTensor(len_hypo_list), torch.from_numpy(np.array(prem_list)), torch.LongTensor(len_prem_list),torch.LongTensor(label_list)]\n",
    "\n",
    "def test_model(data_loader, model, interaction_type):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "\n",
    "    return:\n",
    "    accuracy, loss\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(data_loader):\n",
    "        outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem, interaction_type)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        # Compute acc\n",
    "        outputs_softmax = F.softmax(outputs, dim=1)\n",
    "        predicted = outputs_softmax.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), loss.item()\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "def build_vocab(hypo_tokens, prem_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "\n",
    "    hypo_token_counter = Counter(hypo_tokens)\n",
    "    prem_token_counter = Counter(prem_tokens)\n",
    "\n",
    "    all_tokens_counter = hypo_token_counter + prem_token_counter\n",
    "\n",
    "    vocab, count = zip(*all_tokens_counter.most_common(max_vocab_size))\n",
    "\n",
    "    # print(all_tokens_counter.most_common(MAX_VOCAB_SIZE))\n",
    "\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2, 2 + len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction\n",
      "travel\n",
      "government\n",
      "slate\n",
      "telephone\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "# this is the paramter of the best model\n",
    "BATCH_SIZE = 1024\n",
    "max_sentence_length = 20\n",
    "max_vocab_size = 50000\n",
    "emb_dim = 50\n",
    "interaction_type = 'concat'\n",
    "#best_model_save = os.getcwd() + '/../3_2/best_checkpoint4_nn.tar'\n",
    "best_model_save = '/Users/ludi/Desktop/tars/best_checkpoint4_nn.tar'\n",
    "\n",
    "genres = ['fiction', 'travel', 'government', 'slate', 'telephone']\n",
    "folder = os.getcwd() + '/../all_data_pickle/'\n",
    "all_hypo_data_tokens_train = pkl.load(open(folder+\"all_hypo_data_tokens_train.p\", \"rb\"))\n",
    "all_prem_data_tokens_train = pkl.load(open(folder+\"all_prem_data_tokens_train.p\", \"rb\"))\n",
    "token2id, id2token = build_vocab(all_hypo_data_tokens_train, all_prem_data_tokens_train, max_vocab_size)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mult_genre_val_acc = {}\n",
    "\n",
    "for genre in genres:\n",
    "    print(genre)\n",
    "    result = {}\n",
    "    hypo_data_tokens_val = pkl.load(open(folder+\"hypo_data_tokens_val_{}.p\".format(genre), \"rb\"))\n",
    "    prem_data_tokens_val = pkl.load(open(folder+\"prem_data_tokens_val_{}.p\".format(genre), \"rb\"))\n",
    "    hypo_data_tokens_train = pkl.load(open(folder+\"hypo_data_tokens_train_{}.p\".format(genre), \"rb\"))\n",
    "    prem_data_tokens_train = pkl.load(open(folder+\"prem_data_tokens_train_{}.p\".format(genre), \"rb\"))\n",
    "    label_index_train = pkl.load(open(folder+\"label_index_train_{}.p\".format(genre), \"rb\"))\n",
    "    label_index_val = pkl.load(open(folder+\"label_index_val_{}.p\".format(genre), \"rb\"))\n",
    "    \n",
    "    hypo_data_indices_train = token2index(hypo_data_tokens_train)\n",
    "    prem_data_indices_train = token2index(prem_data_tokens_train)\n",
    "    hypo_data_indices_val = token2index(hypo_data_tokens_val)\n",
    "    prem_data_indices_val= token2index(prem_data_tokens_val)\n",
    "\n",
    "    \n",
    "    train_dataset = NewsGroupDataset(hypo_data_indices_train, prem_data_indices_train, label_index_train, max_sentence_length)\n",
    "    val_dataset = NewsGroupDataset(hypo_data_indices_val, prem_data_indices_val, label_index_val,max_sentence_length)\n",
    "    # seperate dataset into different batch\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, collate_fn=newsgroup_collate_func,shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE,collate_fn=newsgroup_collate_func,shuffle=True)\n",
    "    \n",
    "    # fine tune parameter\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 20\n",
    "    \n",
    "    #initialize model\n",
    "    model = NeuralNetworkPytorch(len(id2token), emb_dim, len(set(label_index_train)), interaction_type)\n",
    "    checkpoint = torch.load(best_model_save)\n",
    "    # load the pretrained model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # for optimizer: add arg \"weight_decay\" for regularizer (float, optional) – weight decay (L2 penalty) (default: 0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_losses = [] # record training loss after every batch\n",
    "    val_accs = [] # record validation accuracy after every batch\n",
    "    # Save state_dict for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_acc_val = 0.0\n",
    "        running_loss_train = 0.0\n",
    "        for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem, interaction_type)\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs,labels)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ### training\n",
    "            # training loss\n",
    "            train_loss = loss.item()\n",
    "\n",
    "            ### validation would be implement in each time (batch)\n",
    "            val_acc, val_loss = test_model(val_loader, model, interaction_type)\n",
    "            running_acc_val += val_acc\n",
    "            running_loss_train += train_loss\n",
    "            \n",
    "        val_accs.append(running_acc_val/len(train_loader))\n",
    "        train_losses.append(running_loss_train/len(train_loader))\n",
    "    \n",
    "    result = {'train_losses': train_losses, 'val_accs': val_accs}\n",
    "    mult_genre_val_acc[genre]=result\n",
    "\n",
    "pkl.dump(mult_genre_val_acc, open(\"3_3_tuned_NN_val_acc.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best val acc on genre fiction is 46.40703517587939.\n",
      "best val acc on genre travel is 49.669042769857434.\n",
      "best val acc on genre government is 49.360236220472444.\n",
      "best val acc on genre slate is 41.91616766467066.\n",
      "best val acc on genre telephone is 50.82587064676617.\n"
     ]
    }
   ],
   "source": [
    "for key, val in mult_genre_val_acc.items():\n",
    "    print('best val acc on genre {} is {}.'.format(key, max(mult_genre_val_acc[key]['val_accs'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction': {'train_losses': [1.1398459374904633,\n",
       "   1.0210418403148651,\n",
       "   0.9856805801391602,\n",
       "   0.9248123019933701,\n",
       "   0.853766679763794,\n",
       "   0.7808659076690674,\n",
       "   0.691236674785614,\n",
       "   0.595949649810791,\n",
       "   0.48424553871154785,\n",
       "   0.3795143887400627,\n",
       "   0.279567688703537,\n",
       "   0.18764645978808403,\n",
       "   0.12083271145820618,\n",
       "   0.0709187313914299,\n",
       "   0.0380311026237905,\n",
       "   0.01979431975632906,\n",
       "   0.010978140635415912,\n",
       "   0.0055044207838363945,\n",
       "   0.0037259236560203135,\n",
       "   0.004014887847006321],\n",
       "  'val_accs': [43.69346733668342,\n",
       "   41.88442211055276,\n",
       "   42.814070351758794,\n",
       "   45.32663316582914,\n",
       "   46.40703517587939,\n",
       "   45.12562814070352,\n",
       "   43.618090452261306,\n",
       "   44.2713567839196,\n",
       "   44.77386934673367,\n",
       "   43.84422110552764,\n",
       "   43.69346733668342,\n",
       "   44.07035175879397,\n",
       "   44.02010050251256,\n",
       "   44.02010050251257,\n",
       "   44.04522613065327,\n",
       "   43.99497487437186,\n",
       "   43.66834170854271,\n",
       "   43.81909547738694,\n",
       "   43.618090452261306,\n",
       "   42.93969849246231]},\n",
       " 'travel': {'train_losses': [1.1145970523357391,\n",
       "   0.9937233179807663,\n",
       "   0.948337584733963,\n",
       "   0.8822244703769684,\n",
       "   0.7983298450708389,\n",
       "   0.7191683202981949,\n",
       "   0.6228091716766357,\n",
       "   0.5199448764324188,\n",
       "   0.41045643389225006,\n",
       "   0.3045542240142822,\n",
       "   0.20791147649288177,\n",
       "   0.1286528930068016,\n",
       "   0.06996848899871111,\n",
       "   0.035698787309229374,\n",
       "   0.017807631753385067,\n",
       "   0.009921662276610732,\n",
       "   0.004840244073420763,\n",
       "   0.002796620043227449,\n",
       "   0.0016568745486438274,\n",
       "   0.0009519959712633863],\n",
       "  'val_accs': [46.94501018329939,\n",
       "   45.21384928716904,\n",
       "   46.71588594704684,\n",
       "   47.9633401221996,\n",
       "   48.70162932790224,\n",
       "   48.26883910386965,\n",
       "   48.72708757637474,\n",
       "   48.54887983706721,\n",
       "   47.836048879837065,\n",
       "   48.34521384928716,\n",
       "   48.62525458248473,\n",
       "   48.77800407331976,\n",
       "   49.0071283095723,\n",
       "   49.516293279022406,\n",
       "   49.669042769857434,\n",
       "   49.18533604887983,\n",
       "   48.90529531568229,\n",
       "   48.80346232179226,\n",
       "   48.75254582484725,\n",
       "   48.62525458248473]},\n",
       " 'government': {'train_losses': [1.0887247920036316,\n",
       "   0.971330851316452,\n",
       "   0.918676108121872,\n",
       "   0.8541069328784943,\n",
       "   0.7764468938112259,\n",
       "   0.6942459493875504,\n",
       "   0.5962747931480408,\n",
       "   0.5019411891698837,\n",
       "   0.4076846241950989,\n",
       "   0.3128529116511345,\n",
       "   0.23263829573988914,\n",
       "   0.16345496103167534,\n",
       "   0.10636077262461185,\n",
       "   0.06613209377974272,\n",
       "   0.03831317275762558,\n",
       "   0.020132727455347776,\n",
       "   0.012882843846455216,\n",
       "   0.008157598203979433,\n",
       "   0.00696255115326494,\n",
       "   0.007461631495971233],\n",
       "  'val_accs': [46.161417322834644,\n",
       "   47.73622047244094,\n",
       "   47.90846456692914,\n",
       "   47.81003937007874,\n",
       "   48.27755905511811,\n",
       "   49.04035433070867,\n",
       "   48.74507874015748,\n",
       "   49.18799212598425,\n",
       "   49.08956692913386,\n",
       "   48.326771653543304,\n",
       "   48.892716535433074,\n",
       "   48.72047244094488,\n",
       "   48.868110236220474,\n",
       "   49.21259842519685,\n",
       "   49.04035433070867,\n",
       "   48.794291338582674,\n",
       "   48.769685039370074,\n",
       "   48.425196850393704,\n",
       "   49.360236220472444,\n",
       "   48.62204724409449]},\n",
       " 'slate': {'train_losses': [1.1712473332881927,\n",
       "   1.0424500107765198,\n",
       "   1.0161739587783813,\n",
       "   0.9674205332994461,\n",
       "   0.9155716300010681,\n",
       "   0.8409314900636673,\n",
       "   0.7619579881429672,\n",
       "   0.6671139150857925,\n",
       "   0.5579698234796524,\n",
       "   0.4424944669008255,\n",
       "   0.33243294060230255,\n",
       "   0.22695794701576233,\n",
       "   0.1428538467735052,\n",
       "   0.08145375736057758,\n",
       "   0.04154705349355936,\n",
       "   0.021094469586387277,\n",
       "   0.011699640890583396,\n",
       "   0.0069741104962304235,\n",
       "   0.004167442908510566,\n",
       "   0.0040334410150535405],\n",
       "  'val_accs': [39.69560878243513,\n",
       "   37.4500998003992,\n",
       "   39.02195608782435,\n",
       "   39.44610778443114,\n",
       "   40.5189620758483,\n",
       "   40.7185628742515,\n",
       "   41.91616766467066,\n",
       "   41.26746506986028,\n",
       "   41.26746506986028,\n",
       "   41.14271457085828,\n",
       "   41.26746506986028,\n",
       "   40.14471057884232,\n",
       "   39.92015968063872,\n",
       "   40.41916167664671,\n",
       "   40.6936127744511,\n",
       "   39.79540918163673,\n",
       "   39.49600798403194,\n",
       "   39.970059880239525,\n",
       "   39.92015968063872,\n",
       "   39.69560878243513]},\n",
       " 'telephone': {'train_losses': [1.1643736362457275,\n",
       "   1.041163396835327,\n",
       "   1.014541471004486,\n",
       "   0.9493159532546998,\n",
       "   0.8934048295021058,\n",
       "   0.8276910185813904,\n",
       "   0.7467617869377137,\n",
       "   0.6816959261894227,\n",
       "   0.5811257898807526,\n",
       "   0.5055919170379639,\n",
       "   0.4223308742046356,\n",
       "   0.3419483542442322,\n",
       "   0.26561291217803956,\n",
       "   0.21129728555679322,\n",
       "   0.1455653116106987,\n",
       "   0.10442033112049103,\n",
       "   0.08044553250074386,\n",
       "   0.05037160590291023,\n",
       "   0.039224547892808916,\n",
       "   0.02227279208600521],\n",
       "  'val_accs': [45.791044776119406,\n",
       "   44.67661691542288,\n",
       "   46.507462686567166,\n",
       "   48.03980099502487,\n",
       "   50.746268656716424,\n",
       "   50.82587064676617,\n",
       "   50.70646766169155,\n",
       "   49.53233830845771,\n",
       "   48.45771144278607,\n",
       "   47.582089552238806,\n",
       "   48.11940298507462,\n",
       "   48.199004975124375,\n",
       "   47.02487562189055,\n",
       "   46.58706467661692,\n",
       "   47.1044776119403,\n",
       "   47.20398009950249,\n",
       "   47.28358208955224,\n",
       "   47.124378109452735,\n",
       "   47.20398009950248,\n",
       "   46.62686567164179]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult_genre_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
