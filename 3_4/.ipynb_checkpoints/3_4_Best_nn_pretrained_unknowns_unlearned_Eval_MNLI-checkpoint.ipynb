{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evaluating on MultiNLI (Best Neural Network Model with pretrained word embedding, unknowns unlearned)\n",
    "\n",
    "**ONLY VALIDATION DATA ARE USED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating on fiction genre...\n",
      "The best nn model using pretrained word embedding and unknowns unlearned validation accuracy on fiction genre is 44.02.\n",
      "Validating on travel genre...\n",
      "The best nn model using pretrained word embedding and unknowns unlearned validation accuracy on travel genre is 44.4.\n",
      "Validating on government genre...\n",
      "The best nn model using pretrained word embedding and unknowns unlearned validation accuracy on government genre is 44.78.\n",
      "Validating on slate genre...\n",
      "The best nn model using pretrained word embedding and unknowns unlearned validation accuracy on slate genre is 42.12.\n",
      "Validating on telephone genre...\n",
      "The best nn model using pretrained word embedding and unknowns unlearned validation accuracy on telephone genre is 41.59.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#####################################################\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import pickle as pkl\n",
    "import os\n",
    "######################################################\n",
    "## Hyper paramter\n",
    "max_vocab_size = 100000\n",
    "emb_dim = 300\n",
    "interaction_type = 'concat'\n",
    "learning_rate = 0.01\n",
    "h1_dim = 90\n",
    "h2_dim = 90\n",
    "num_epochs = 17 # optimal epoch\n",
    "BATCH_SIZE = 1024\n",
    "filename = \"BEST_unknowns_unlearned_model\"\n",
    "######################################################\n",
    "## All used data is in the following folder\n",
    "folder = os.getcwd() + '/../all_data_pickle/'\n",
    "######################################################\n",
    "# save index 0 for unk and 1 for pad\n",
    "global PAD_IDX ,UNK_IDX\n",
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "######################################################\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hypo_list, prem_list, target_list, max_sentence_length):\n",
    "        \"\"\"\n",
    "        @param hypo_list: list of hypo tokens\n",
    "        @param prem_list: list of prem tokens\n",
    "        @param target_list: list of newsgroup targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.hypo_list = hypo_list\n",
    "        self.prem_list = prem_list\n",
    "        self.target_list = target_list\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        assert (len(self.hypo_list) == len(self.target_list))\n",
    "        assert (len(self.prem_list) == len(self.target_list))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hypo_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "\n",
    "        token_hypo_idx = self.hypo_list[key][:self.max_sentence_length]\n",
    "        token_prem_idx = self.prem_list[key][:self.max_sentence_length]\n",
    "        label = self.target_list[key]\n",
    "        return [token_hypo_idx, len(token_hypo_idx), token_prem_idx, len(token_prem_idx), label]\n",
    "\n",
    "############################################################################\n",
    "# Any change about model should be here [interaction_type]\n",
    "############################################################################\n",
    "class NeuralNetworkPytorch(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_out, interaction_type):\n",
    "        super(NeuralNetworkPytorch, self).__init__()\n",
    "\n",
    "        if interaction_type == 'concat':\n",
    "            emb_dim *= 2\n",
    "\n",
    "        self.embed = nn.EmbeddingBag.from_pretrained(weight,freeze=True)\n",
    "        self.emb_h1 = nn.Linear(emb_dim, h1_dim)\n",
    "        self.dp = nn.Dropout(p = 0.2)\n",
    "        self.h1_h2 = nn.Linear(h1_dim, h2_dim)\n",
    "        self.h2_out = nn.Linear(h2_dim, 3)\n",
    "\n",
    "    def forward(self, data_hypo, length_hypo, data_prem, length_prem, interaction_type):\n",
    "        \"\"\"\n",
    "            @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a\n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "            @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            @param data_prem: matrix of size (batch_size, max_sentence_length).\n",
    "            @param length_hypo: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "                length of each sentences in the data_prem.\n",
    "            @param interaction_type: [sum. hadamart, concat]\n",
    "        \"\"\"\n",
    "\n",
    "        x_hypo=data_hypo\n",
    "        x_prem=data_prem\n",
    "        embed_hypo = self.embed(x_hypo)\n",
    "        embed_prem = self.embed(x_prem)\n",
    "\n",
    "        if interaction_type == 'concat':\n",
    "            x = torch.cat((embed_hypo,embed_prem), 1)\n",
    "        if interaction_type == 'sum':\n",
    "            x = torch.add(embed_hypo,embed_prem)\n",
    "        if interaction_type == 'hadamard':\n",
    "            x = embed_hypo * embed_prem\n",
    "\n",
    "        x = F.relu(self.emb_h1(x.float()))\n",
    "        x = F.relu(self.h1_h2(x))\n",
    "        y = self.h2_out(x)\n",
    "        return y\n",
    "\n",
    "def build_vocab(hypo_tokens, prem_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "\n",
    "    #hypo_token_counter = Counter(hypo_tokens)\n",
    "    #prem_token_counter = Counter(prem_tokens)\n",
    "\n",
    "    #all_tokens_counter = hypo_token_counter + prem_token_counter\n",
    "\n",
    "    #vocab, count = zip(*all_tokens_counter.most_common(max_vocab_size))\n",
    "\n",
    "    #print(all_tokens_counter.most_common(MAX_VOCAB_SIZE))\n",
    "\n",
    "    id2token = list(f[\",\"])\n",
    "    token2id = dict(zip(f[\",\"], range(2,2+len(f))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    hypo_list = []\n",
    "    len_hypo_list = []\n",
    "    prem_list = []\n",
    "    len_prem_list = []\n",
    "    label_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch)\n",
    "    #batch[0][0] = batch[0][0][:max_sentence_length_prem]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        len_hypo_list.append(datum[1])\n",
    "        len_prem_list.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        # hypo\n",
    "        padded_vec = np.pad(np.array(datum[0]), pad_width=((0,max_sentence_length-datum[1])), mode=\"constant\", constant_values=0)\n",
    "        hypo_list.append(padded_vec)\n",
    "        # prem\n",
    "        padded_vec = np.pad(np.array(datum[2]), pad_width=((0,max_sentence_length-datum[3])), mode=\"constant\", constant_values=0)\n",
    "        prem_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(hypo_list)), torch.LongTensor(len_hypo_list), torch.from_numpy(np.array(prem_list)), torch.LongTensor(len_prem_list),torch.LongTensor(label_list)]\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(data_loader, model, interaction_type):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(data_loader):\n",
    "        outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem, interaction_type)\n",
    "        #compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        #compute acc\n",
    "        outputs_softmax = F.softmax(outputs, dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), loss.item()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # load word_vector_dictionary\n",
    "\n",
    "    f=pkl.load(open(\"/Users/ludi/Desktop/tars/f.p\",\"rb\"))\n",
    "    f=f[:max_vocab_size]\n",
    "    weight=pkl.load(open(\"/Users/ludi/Desktop/tars/weights.p\",\"rb\"))\n",
    "    weight=weight[:max_vocab_size]\n",
    "    weight=torch.FloatTensor(weight)\n",
    "\n",
    "    # load data and tokens\n",
    "    hypo_data_tokens_train = pkl.load(open(folder+\"hypo_data_tokens_train.p\", \"rb\"))\n",
    "    prem_data_tokens_train = pkl.load(open(folder+\"prem_data_tokens_train.p\", \"rb\"))\n",
    "    hypo_data_tokens_val = pkl.load(open(folder+\"hypo_data_tokens_val.p\", \"rb\"))\n",
    "    prem_data_tokens_val = pkl.load(open(folder+\"prem_data_tokens_val.p\", \"rb\"))\n",
    "\n",
    "    all_hypo_data_tokens_train = pkl.load(open(folder+\"all_hypo_data_tokens_train.p\", \"rb\"))\n",
    "    all_prem_data_tokens_train = pkl.load(open(folder+\"all_prem_data_tokens_train.p\", \"rb\"))\n",
    "    all_hypo_data_tokens_val = pkl.load(open(folder+\"all_hypo_data_tokens_val.p\", \"rb\"))\n",
    "    all_prem_data_tokens_val = pkl.load(open(folder+\"all_prem_data_tokens_val.p\", \"rb\"))\n",
    "\n",
    "    # Vocabulary\n",
    "    # buid vocabulary index token accodding to max_vocab_size\n",
    "    token2id, id2token = build_vocab(all_hypo_data_tokens_train, all_prem_data_tokens_train, max_vocab_size)\n",
    "    hypo_data_indices_train = token2index(hypo_data_tokens_train)\n",
    "    prem_data_indices_train = token2index(prem_data_tokens_train)\n",
    "    hypo_data_indices_val = token2index(hypo_data_tokens_val)\n",
    "    prem_data_indices_val= token2index(prem_data_tokens_val)\n",
    "\n",
    "    label_index_train = pkl.load(open(folder+\"label_index_train.p\", \"rb\"))\n",
    "    label_index_val = pkl.load(open(folder+\"label_index_val.p\", \"rb\"))\n",
    "    \n",
    "     # # PyTorch DataLoader\n",
    "    max_sentence_length = 20\n",
    "    \n",
    "    genres = ['fiction', 'travel', 'government', 'slate', 'telephone']\n",
    "    mult_genre_val_acc = {}\n",
    "    for genre in genres:\n",
    "        hypo_data_tokens_val = pkl.load(open(folder+\"hypo_data_tokens_val_{}.p\".format(genre), \"rb\"))\n",
    "        prem_data_tokens_val = pkl.load(open(folder+\"prem_data_tokens_val_{}.p\".format(genre), \"rb\"))\n",
    "        label_index_val = pkl.load(open(folder+\"label_index_val_{}.p\".format(genre), \"rb\"))\n",
    "        hypo_data_indices_val = token2index(hypo_data_tokens_val)\n",
    "        prem_data_indices_val= token2index(prem_data_tokens_val)\n",
    "        \n",
    "        val_dataset = NewsGroupDataset(hypo_data_indices_val, prem_data_indices_val, label_index_val,max_sentence_length)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,collate_fn=newsgroup_collate_func,shuffle=True)\n",
    "        model = NeuralNetworkPytorch(len(id2token), emb_dim, len(set(label_index_val)), interaction_type)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_model_save = '/Users/ludi/Desktop/tars/best_nn_pretrained_unknowns_unlearned_state_dict.tar'\n",
    "        checkpoint = torch.load(best_model_save)\n",
    "        # load the pretrained model\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        print('Validating on {} genre...'.format(genre))\n",
    "        val_acc, val_loss = test_model(val_loader, model, interaction_type)\n",
    "        mult_genre_val_acc[genre] = val_acc\n",
    "        print('The best nn model using pretrained word embedding and unknowns unlearned validation accuracy on {} genre is {}.'.format(genre, np.around(val_acc,2)))\n",
    "\n",
    "    pkl.dump(mult_genre_val_acc, open(\"3_4_best_nn_pretrained_unknowns_unlearned_Eval_On_MNLI.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
