{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evaluating on MultiNLI (Best Neural Network Model with pretrained word embedding, unknowns learned)\n",
    "\n",
    "**ONLY VALIDATION DATA ARE USED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating on fiction genre...\n",
      "The best nn model using pretrained word embedding and unknowns learned validation accuracy on fiction genre is 42.71.\n",
      "Validating on travel genre...\n",
      "The best nn model using pretrained word embedding and unknowns learned validation accuracy on travel genre is 43.89.\n",
      "Validating on government genre...\n",
      "The best nn model using pretrained word embedding and unknowns learned validation accuracy on government genre is 42.52.\n",
      "Validating on slate genre...\n",
      "The best nn model using pretrained word embedding and unknowns learned validation accuracy on slate genre is 41.82.\n",
      "Validating on telephone genre...\n",
      "The best nn model using pretrained word embedding and unknowns learned validation accuracy on telephone genre is 43.68.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "######################################################\n",
    "## Hyper paramter\n",
    "\n",
    "max_vocab_size = 100000\n",
    "emb_dim = 300\n",
    "interaction_type= \"concat\"\n",
    "\n",
    "######################################################\n",
    "learning_rate = 0.01\n",
    "num_epochs = 17 # number epoch to train\n",
    "BATCH_SIZE = 1024\n",
    "filename = \"BEST_unknowns_learned_model\"\n",
    "######################################################\n",
    "# save index 0 for unk and 1 for pad\n",
    "global PAD_IDX ,UNK_IDX\n",
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hypo_list, prem_list, target_list, max_sentence_length):\n",
    "        \"\"\"\n",
    "        @param hypo_list: list of hypo tokens\n",
    "        @param prem_list: list of prem tokens\n",
    "        @param target_list: list of newsgroup targets\n",
    "        @param max_sentence_length: fixed length of all sentence\n",
    "\n",
    "        \"\"\"\n",
    "        self.hypo_list = hypo_list\n",
    "        self.prem_list = prem_list\n",
    "        self.target_list = target_list\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        assert (len(self.hypo_list) == len(self.target_list))\n",
    "        assert (len(self.prem_list) == len(self.target_list))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hypo_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "            Triggered when you call dataset[i]\n",
    "            \"\"\"\n",
    "\n",
    "        token_hypo_idx = self.hypo_list[key][:self.max_sentence_length]\n",
    "        token_prem_idx = self.prem_list[key][:self.max_sentence_length]\n",
    "        label = self.target_list[key]\n",
    "        return [token_hypo_idx, len(token_hypo_idx), token_prem_idx, len(token_prem_idx), label]\n",
    "\n",
    "############################################################################\n",
    "# Any change about model should be here [interaction_type]\n",
    "############################################################################\n",
    "class NeuralNetworkPytorch(nn.Module):\n",
    "    \"\"\"\n",
    "    NeuralNetwork classification model\n",
    "    Model would change according to interaction_type\n",
    "\n",
    "    1st hidden layer: 90 neurons\n",
    "    2nd hidden layer: 90 neurons\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, n_out, interaction_type):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary.\n",
    "        @param emb_dim: size of the word embedding\n",
    "        @param n_out: size of the class.\n",
    "        \"\"\"\n",
    "        super(NeuralNetworkPytorch, self).__init__()\n",
    "\n",
    "        # 1. Embedding\n",
    "        self.embed = nn.EmbeddingBag.from_pretrained(weight,freeze=True)\n",
    "\n",
    "\n",
    "        # 2. an affine operation: y=Wx+b\n",
    "        # double embedding dimension if we concat hypo's and prem's embedding\n",
    "        if interaction_type == 'concat':\n",
    "            emb_dim = 2 * emb_dim\n",
    "        self.hidden_1= nn.Linear(emb_dim,90)\n",
    "        self.hidden_2=nn.Linear(90, 90)\n",
    "        self.output = nn.Linear(90, n_out)\n",
    "\n",
    "    def forward(self, data_hypo, length_hypo, data_prem, length_prem, interaction_type):\n",
    "        \"\"\"\n",
    "            @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a\n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "            @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            @param data_prem: matrix of size (batch_size, max_sentence_length).\n",
    "            @param length_hypo: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "                length of each sentences in the data_prem.\n",
    "            @param interaction_type: [sum. hadamart, concat]\n",
    "            \"\"\"\n",
    "        # word embedding\n",
    "        x_hypo=data_hypo # 1024 x 20\n",
    "        x_prem=data_prem\n",
    "        embed_hypo = self.embed(x_hypo) # 1024 x 300\n",
    "        embed_prem = self.embed(x_prem)\n",
    "        m_hypo = (x_hypo == 1) # 1024 x 20 take all unknowns out of x_hypo\n",
    "        m_prem= (x_prem == 1)\n",
    "        m_hypo = m_hypo.unsqueeze(2).repeat(1, 1, 300).type(torch.FloatTensor)#.to(device) # 1024 x 20 x 300\n",
    "        m_prem = m_prem.unsqueeze(2).repeat(1, 1, 300).type(torch.FloatTensor)#.to(device)\n",
    "        m_hypo=torch.sum(m_hypo,dim=1) # 1024 x 300\n",
    "        m_prem=torch.sum(m_prem,dim=1)\n",
    "        m_hypo /= length_hypo.view(length_hypo.size()[0],1).expand_as(m_hypo).float()\n",
    "        m_prem /= length_prem.view(length_prem.size()[0],1).expand_as(m_prem).float()\n",
    "        \n",
    "        out_hypo = m_hypo * embed_hypo + (1-m_hypo) * embed_hypo.clone().detach()\n",
    "        out_prem = m_prem * embed_prem + (1-m_prem) * embed_prem.clone().detach()\n",
    "\n",
    "        # interaction\n",
    "        # 1. sum\n",
    "        # 2. Hadamard product\n",
    "        # 3. concat (This will change embedding dimension, 2 times as many as before)\n",
    "        if interaction_type == 'concat':\n",
    "            out = torch.cat((out_hypo,out_prem),1)\n",
    "        if interaction_type == 'sum':\n",
    "            out = torch.add(out_hypo, out_prem)\n",
    "        if interaction_type == 'hadamard':\n",
    "            out = out_hypo * out_prem\n",
    "\n",
    "        out = self.hidden_1(out.float())\n",
    "        out = F.relu(out)\n",
    "        out = self.hidden_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "def build_vocab(hypo_tokens, prem_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "\n",
    "    #hypo_token_counter = Counter(hypo_tokens)\n",
    "    #prem_token_counter = Counter(prem_tokens)\n",
    "\n",
    "    #all_tokens_counter = hypo_token_counter + prem_token_counter\n",
    "\n",
    "    #vocab, count = zip(*all_tokens_counter.most_common(max_vocab_size))\n",
    "\n",
    "    #print(all_tokens_counter.most_common(MAX_VOCAB_SIZE))\n",
    "\n",
    "    id2token = list(f[\",\"])\n",
    "    token2id = dict(zip(f[\",\"], range(2,2+len(f))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    hypo_list = []\n",
    "    len_hypo_list = []\n",
    "    prem_list = []\n",
    "    len_prem_list = []\n",
    "    label_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch)\n",
    "    #batch[0][0] = batch[0][0][:max_sentence_length_prem]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        len_hypo_list.append(datum[1])\n",
    "        len_prem_list.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        # hypo\n",
    "        padded_vec = np.pad(np.array(datum[0]), pad_width=((0,max_sentence_length-datum[1])), mode=\"constant\", constant_values=0)\n",
    "        hypo_list.append(padded_vec)\n",
    "        # prem\n",
    "        padded_vec = np.pad(np.array(datum[2]), pad_width=((0,max_sentence_length-datum[3])), mode=\"constant\", constant_values=0)\n",
    "        prem_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(hypo_list)), torch.LongTensor(len_hypo_list), torch.from_numpy(np.array(prem_list)), torch.LongTensor(len_prem_list),torch.LongTensor(label_list)]\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(data_loader, model, interaction_type):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "\n",
    "    return:\n",
    "    accuracy, loss\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(data_loader):\n",
    "        outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem, interaction_type)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Compute acc\n",
    "        outputs_softmax = F.softmax(outputs, dim=1)\n",
    "        predicted = outputs_softmax.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), loss.item()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    f=pkl.load(open(\"/Users/ludi/Desktop/tars/f.p\",\"rb\"))\n",
    "    f=f[:max_vocab_size]\n",
    "    weight=pkl.load(open(\"/Users/ludi/Desktop/tars/weights.p\",\"rb\"))\n",
    "    weight=weight[:max_vocab_size]\n",
    "    weight=torch.FloatTensor(weight)\n",
    "    \n",
    "    ######################################################\n",
    "    # read data\n",
    "    ######################################################\n",
    "    # # Tokenizing be done\n",
    "    folder = os.getcwd() + '/../all_data_pickle/'\n",
    "    hypo_data_tokens_train = pkl.load(open(folder+\"hypo_data_tokens_train.p\", \"rb\"))\n",
    "    prem_data_tokens_train = pkl.load(open(folder+\"prem_data_tokens_train.p\", \"rb\"))\n",
    "    hypo_data_tokens_val = pkl.load(open(folder+\"hypo_data_tokens_val.p\", \"rb\"))\n",
    "    prem_data_tokens_val = pkl.load(open(folder+\"prem_data_tokens_val.p\", \"rb\"))\n",
    "    all_hypo_data_tokens_train = pkl.load(open(folder+\"all_hypo_data_tokens_train.p\", \"rb\"))\n",
    "    all_prem_data_tokens_train = pkl.load(open(folder+\"all_prem_data_tokens_train.p\", \"rb\"))\n",
    "    all_hypo_data_tokens_val = pkl.load(open(folder+\"all_hypo_data_tokens_val.p\", \"rb\"))\n",
    "    all_prem_data_tokens_val = pkl.load(open(folder+\"all_prem_data_tokens_val.p\", \"rb\"))\n",
    "    label_index_train = pkl.load(open(folder+\"label_index_train.p\", \"rb\"))\n",
    "    label_index_val = pkl.load(open(folder+\"label_index_val.p\", \"rb\"))\n",
    "\n",
    "    # # Vocabulary\n",
    "\n",
    "    # buid vocabulary index token accodding to max_vocab_size\n",
    "    token2id, id2token = build_vocab(all_hypo_data_tokens_train, all_prem_data_tokens_train, max_vocab_size)\n",
    "    hypo_data_indices_train = token2index(hypo_data_tokens_train)\n",
    "    prem_data_indices_train = token2index(prem_data_tokens_train)\n",
    "    hypo_data_indices_val = token2index(hypo_data_tokens_val)\n",
    "    prem_data_indices_val= token2index(prem_data_tokens_val)\n",
    "\n",
    "\n",
    "    # # PyTorch DataLoader\n",
    "    max_sentence_length = 20\n",
    "    \n",
    "    genres = ['fiction', 'travel', 'government', 'slate', 'telephone']\n",
    "    mult_genre_val_acc = {}\n",
    "    for genre in genres:\n",
    "        hypo_data_tokens_val = pkl.load(open(folder+\"hypo_data_tokens_val_{}.p\".format(genre), \"rb\"))\n",
    "        prem_data_tokens_val = pkl.load(open(folder+\"prem_data_tokens_val_{}.p\".format(genre), \"rb\"))\n",
    "        label_index_val = pkl.load(open(folder+\"label_index_val_{}.p\".format(genre), \"rb\"))\n",
    "        hypo_data_indices_val = token2index(hypo_data_tokens_val)\n",
    "        prem_data_indices_val= token2index(prem_data_tokens_val)\n",
    "        \n",
    "        val_dataset = NewsGroupDataset(hypo_data_indices_val, prem_data_indices_val, label_index_val,max_sentence_length)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,collate_fn=newsgroup_collate_func,shuffle=True)\n",
    "        model = NeuralNetworkPytorch(len(id2token), emb_dim, len(set(label_index_val)), interaction_type)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_model_save = '/Users/ludi/Desktop/tars/best_nn_pretrained_unknowns_learned_state_dict.tar'\n",
    "        checkpoint = torch.load(best_model_save)\n",
    "        # load the pretrained model\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        print('Validating on {} genre...'.format(genre))\n",
    "        val_acc, val_loss = test_model(val_loader, model, interaction_type)\n",
    "        mult_genre_val_acc[genre] = val_acc\n",
    "        print('The best nn model using pretrained word embedding and unknowns learned validation accuracy on {} genre is {}.'.format(genre, np.around(val_acc,2)))\n",
    "\n",
    "    pkl.dump(mult_genre_val_acc, open(\"3_4_best_nn_pretrained_unknowns_learned_Eval_On_MNLI.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
