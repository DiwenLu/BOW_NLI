{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analyzing Most Similar Words and Their Changes in Pre-trained Embedding</h2>\n",
    "\n",
    "<h4>Step 1: Get the vocabulary and dictionary of words</h4>\n",
    "\n",
    "<p>Before this step we have already saved the weights from our best model used before. We read the \"wd_vec_dict_SNLI.p\" pickle file which is our trained embedding used in section 3.1. The \"f.p\" pickle file is the file we used to save the dictionary of pre-trained embedding in fastText.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages that needs to be imported\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import dist\n",
    "from torch import norm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "##We first define some variables and packages that we want to import\n",
    "max_vocab_size = 10000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we also needs to build the vocabulary\n",
    "def build_vocab(hypo_tokens, prem_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "\n",
    "    id2token = list(f[\",\"])\n",
    "    token2id = dict(zip(f[\",\"], range(2,2+len(f))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the trained and pretrained embedding\n",
    "wd_vec_dict = pkl.load(open(\"wd_vec_dict_SNLI.p\", \"rb\"))\n",
    "f=pkl.load(open(\"/Users/ludi/Desktop/tars/f.p\",\"rb\"))\n",
    "f=f[:max_vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and tokens\n",
    "hypo_data_tokens_train = pkl.load(open(folder+\"/../all_data_pickle/hypo_data_tokens_train.p\", \"rb\"))\n",
    "prem_data_tokens_train = pkl.load(open(folder+\"/../all_data_pickle/prem_data_tokens_train.p\", \"rb\"))\n",
    "hypo_data_tokens_val = pkl.load(open(folder+\"/../all_data_pickle/hypo_data_tokens_val.p\", \"rb\"))\n",
    "prem_data_tokens_val = pkl.load(open(folder+\"/../all_data_pickle/prem_data_tokens_val.p\", \"rb\"))\n",
    "\n",
    "all_hypo_data_tokens_train = pkl.load(open(folder+\"/../all_data_pickle/all_hypo_data_tokens_train.p\", \"rb\"))\n",
    "all_prem_data_tokens_train = pkl.load(open(folder+\"/../all_data_pickle/all_prem_data_tokens_train.p\", \"rb\"))\n",
    "all_hypo_data_tokens_val = pkl.load(open(folder+\"/../all_data_pickle/all_hypo_data_tokens_val.p\", \"rb\"))\n",
    "all_prem_data_tokens_val = pkl.load(open(folder+\"/../all_data_pickle/all_prem_data_tokens_val.p\", \"rb\"))\n",
    "\n",
    "# Vocabulary\n",
    "# buid vocabulary index token accodding to max_vocab_size\n",
    "token2id, id2token = build_vocab(all_hypo_data_tokens_train, all_prem_data_tokens_train, max_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 2: Get Pair-wise Distances and Get the Most Similar 10 Pairs<\\h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>At first we need to get a id2vec list to help us get the embedding vector using word indices.<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The build_id_vec function\n",
    "def build_id_vec_function(wd_vec, id_token):\n",
    "    \n",
    "    '''\n",
    "    @param wd_vec: the dictionary that includes tokens and vectors\n",
    "    @param id_token: i.e. id2token\n",
    "    '''\n",
    "    \n",
    "    output = []\n",
    "    for token in id_token:\n",
    "        if token in wd_vec:\n",
    "            output.append(torch.from_numpy(wd_vec[token]))\n",
    "        else:\n",
    "            output.append(None)\n",
    "    return output\n",
    "\n",
    "id2vec = build_id_vec_function(wd_vec_dict, id2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we generate a sorted similarity lists which contains information of dissimilarity of different word pairs.<\\p>\n",
    "    \n",
    "<p>We use Euclidean Distance between embedding vectors. An alternative is to use cosine similarity between word vectors. However, from trials we find that this shows little information of word similarity, so we do not use it.<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000/10002] tokens has been visited.\n",
      "[2000/10002] tokens has been visited.\n",
      "[3000/10002] tokens has been visited.\n",
      "[4000/10002] tokens has been visited.\n",
      "[5000/10002] tokens has been visited.\n",
      "[6000/10002] tokens has been visited.\n",
      "[7000/10002] tokens has been visited.\n",
      "[8000/10002] tokens has been visited.\n",
      "[9000/10002] tokens has been visited.\n",
      "[10000/10002] tokens has been visited.\n"
     ]
    }
   ],
   "source": [
    "# Compare the similarity between tokens\n",
    "def build_similar_pairs(id_vec):\n",
    "    '''\n",
    "    @param id_vec: i.e. id2vec\n",
    "    '''\n",
    "    output = []\n",
    "    for i in range(1, len(id_vec)):\n",
    "        if type(id_vec[i]) != type(None):\n",
    "            for j in range(i+1, len(id_vec)):\n",
    "                if type(id_vec[j]) != type(None):\n",
    "                    output.append((i, j, dist(id_vec[i], id_vec[j], p = 2).item()))\n",
    "                    \n",
    "        ##Let's also see how how the process goes on\n",
    "        if i > 0 and i % 1000 == 999:\n",
    "            print(\"[{}/{}] tokens has been visited.\".format(i+1,len(id_vec)))\n",
    "    \n",
    "    ## At last we want to sort the similarity list\n",
    "    output = sorted(output, key=lambda x: x[2], reverse=False)\n",
    "    \n",
    "    return output\n",
    "\n",
    "similarity_pairs = build_similar_pairs(id2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As we do not want functional words, e.g. \"the\", in our comparison (since we want to compare words that have the meaning), we use <strong>NLTK</strong> <em>stopwords</em> corpus to collect functional words, adding up determiners and numbers, then perform the elimination<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elim_func_words(similar_pairs, id_token):\n",
    "    '''\n",
    "    @param similar_pairs: the sorted list of pairs with similarity scores\n",
    "    @param id_token: i.e. id2token\n",
    "    '''\n",
    "    \n",
    "    stp_wds = list(set(stopwords.words('english')))+['a', 'an']+list(map(str, range(100)))\n",
    "    output = []\n",
    "    i_temp = 0\n",
    "    while len(output) < 10:\n",
    "        id_1, id_2, distance = similar_pairs[i_temp]\n",
    "        if id_token[id_1] not in stp_wds and id_token[id_2] not in stp_wds:\n",
    "            output.append((id_token[id_1], id_token[id_2], distance))\n",
    "        i_temp += 1\n",
    "    \n",
    "    return output\n",
    "\n",
    "most_similar_pairs = elim_func_words(similarity_pairs, id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('young', 'child', 9.58918285369873),\n",
       " ('landing', 'cluster', 9.736421585083008),\n",
       " ('man', 'guys', 9.81108283996582),\n",
       " ('group', 'hat', 9.829873085021973),\n",
       " ('young', 'casual', 10.05659008026123),\n",
       " ('group', 'wonderful', 10.079551696777344),\n",
       " ('young', 'grass', 10.100854873657227),\n",
       " ('man', 'sites', 10.119364738464355),\n",
       " ('young', 'across', 10.165505409240723),\n",
       " ('area', 'young', 10.191558837890625)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Step 3: See Their Representation in Pre-train Embeddings<\\h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this step we compute again the pair-wise distance in pretrained embedding.<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_dist(pretrained_dict, most_similar):\n",
    "    '''\n",
    "    @param pretrained_dict: the dictionary of pre-trained embeddings\n",
    "    @param most_similar: the 10 most similar pairs we get in step 2\n",
    "    '''\n",
    "    \n",
    "    pretrained_wd_vec = {}\n",
    "    for i in range(len(pretrained_dict[','])):\n",
    "        pretrained_wd_vec[pretrained_dict.iloc[i][0]] = pretrained_dict.iloc[i][1]\n",
    "    \n",
    "    output = []\n",
    "    for tk_1, tk_2,_ in most_similar:\n",
    "        pre_train_dist = dist(torch.tensor(pretrained_wd_vec[tk_1])\n",
    "                            , torch.tensor(pretrained_wd_vec[tk_2])\n",
    "                            , p = 2).item()\n",
    "        output.append((tk_1, tk_2, pre_train_dist))\n",
    "    \n",
    "    return output\n",
    "\n",
    "pretrained_distance = get_pretrained_dist(f, most_similar_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('young', 'child', 1.8071242570877075),\n",
       " ('landing', 'cluster', 2.6011438369750977),\n",
       " ('man', 'guys', 1.7539427280426025),\n",
       " ('group', 'hat', 2.304748296737671),\n",
       " ('young', 'casual', 2.0644662380218506),\n",
       " ('group', 'wonderful', 2.1527419090270996),\n",
       " ('young', 'grass', 2.329569101333618),\n",
       " ('man', 'sites', 2.251317262649536),\n",
       " ('young', 'across', 2.2015292644500732),\n",
       " ('area', 'young', 1.8896145820617676)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ludi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
