{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all train data\n",
    "all_raw_data_train = pd.read_csv(path + \"/SNLI_MNLI_data/snli_train.tsv\", sep='\\t')\n",
    "hypo_train=all_raw_data_train[\"sentence1\"]\n",
    "prem_train=all_raw_data_train[\"sentence2\"]\n",
    "label_train=all_raw_data_train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all validation data\n",
    "all_raw_data_val = pd.read_csv(path + \"/SNLI_MNLI_data/snli_val.tsv\", sep='\\t')\n",
    "hypo_val=all_raw_data_val[\"sentence1\"]\n",
    "prem_val=all_raw_data_val[\"sentence2\"]\n",
    "label_val=all_raw_data_val[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d ={'neutral': 0, 'entailment': 1, 'contradiction': 2}\n",
    "label_index=label_train.map(d, na_action='ignore')\n",
    "label_index_val=label_val.map(d, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 3 columns):\n",
      "sentence1    100000 non-null object\n",
      "sentence2    100000 non-null object\n",
      "label        100000 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.3+ MB\n",
      "snli_train dataset size is 100000\n"
     ]
    }
   ],
   "source": [
    "# have a glance at the train data\n",
    "all_raw_data_train.info()\n",
    "print(\"snli_train dataset size is {}\".format(len(all_raw_data_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 72305 distinct hypothesis in training data.\n",
      "There are 93549 distinct premise in training data.\n",
      "The 3 labels are {'neutral', 'entailment', 'contradiction'}.\n"
     ]
    }
   ],
   "source": [
    "# have a glance at the train hypothese and premise\n",
    "print(\"There are {} distinct hypothesis in training data.\".format(len(hypo_train.unique())))\n",
    "print(\"There are {} distinct premise in training data.\".format(len(prem_train.unique())))\n",
    "print(\"The 3 labels are {}.\".format(set(label_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      "sentence1    1000 non-null object\n",
      "sentence2    1000 non-null object\n",
      "label        1000 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 23.6+ KB\n",
      "snli_train dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# have a glance at the validation data\n",
    "all_raw_data_val.info()\n",
    "print(\"snli_train dataset size is {}\".format(len(all_raw_data_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 891 distinct hypothesis in validation data.\n",
      "There are 999 distinct premise in validation data.\n",
      "The 3 labels are {'neutral', 'entailment', 'contradiction'}.\n"
     ]
    }
   ],
   "source": [
    "# have a glance at the validation hypothese and premise\n",
    "print(\"There are {} distinct hypothesis in validation data.\".format(len(hypo_val.unique())))\n",
    "print(\"There are {} distinct premise in validation data.\".format(len(prem_val.unique())))\n",
    "print(\"The 3 labels are {}.\".format(set(label_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1    Three women , all dressed very stylishly in bl...\n",
      "sentence2                            Three women are indoors .\n",
      "label                                               entailment\n",
      "Name: 29949, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Random sample from train dataset\n",
    "import random\n",
    "t=random.randint(0, len(all_raw_data_train) - 1)\n",
    "print (all_raw_data_train.iloc[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import string\n",
    "\n",
    "tokenizer=spacy.load(\"en_core_web_sm\") # What does this do?\n",
    "punctuations=string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text.lower() for token in tokens if (token.text not in punctuations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing hypothesis_train\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-86d51b63db12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#train set tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokenizing hypothesis_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mhypo_data_tokens_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_hypo_data_tokens_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypo_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypo_data_tokens_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hypo_data_tokens_train.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_hypo_data_tokens_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all_hypo_data_tokens_train.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-86d51b63db12>\u001b[0m in \u001b[0;36mtokenize_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtoken_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mall_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3d66f9cd1fc1>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# lowercase and remove punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(seqs_in)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mXh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/thinc/neural/_classes/maxout.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X__BI)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mX__BOP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX__BI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mX__BOP\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mX__BOP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX__BOP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX__BOP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "#train set tokens\n",
    "print (\"Tokenizing hypothesis_train\")\n",
    "hypo_data_tokens_train, all_hypo_data_tokens_train = tokenize_dataset(hypo_train)\n",
    "pkl.dump(hypo_data_tokens_train, open(\"/all_data_pickle/hypo_data_tokens_train.p\", \"wb\"))\n",
    "pkl.dump(all_hypo_data_tokens_train, open(\"/all_data_pickle/all_hypo_data_tokens_train.p\", \"wb\"))\n",
    "\n",
    "\n",
    "print (\"Tokenizing premise_train\")\n",
    "prem_data_tokens_train, all_prem_data_tokens_train = tokenize_dataset(prem_train)\n",
    "pkl.dump(prem_data_tokens_train, open(\"/all_data_pickle/prem_data_tokens_train.p\", \"wb\"))\n",
    "pkl.dump(all_prem_data_tokens_train, open(\"/all_data_pickle/all_prem_data_tokens_train.p\", \"wb\"))\n",
    "\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing hypothesis_val\")\n",
    "hypo_data_tokens_val, all_hypo_data_tokens_val = tokenize_dataset(hypo_val)\n",
    "pkl.dump(hypo_data_tokens_val, open(\"/all_data_pickle/hypo_data_tokens_val.p\", \"wb\"))\n",
    "pkl.dump(all_hypo_data_tokens_val, open(\"/all_data_pickle/all_hypo_data_tokens_val.p\", \"wb\"))\n",
    "\n",
    "\n",
    "print (\"Tokenizing premise_val\")\n",
    "prem_data_tokens_val, all_prem_data_tokens_val = tokenize_dataset(prem_val)\n",
    "pkl.dump(prem_data_tokens_val, open(\"/all_data_pickle/prem_data_tokens_val.p\", \"wb\"))\n",
    "pkl.dump(all_prem_data_tokens_val, open(\"/all_data_pickle/all_prem_data_tokens_val.p\", \"wb\"))\n",
    "\n",
    "#test set tokens\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "#pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "#train set tokens\n",
    "#print (\"Tokenizing train data\")\n",
    "#train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "#pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "#pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy \n",
    "import string\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "path = os.getcwd() + '/data/'\n",
    "# read all train data\n",
    "data_train = pd.read_csv(path + \"mnli_train.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing hypothesis_train\n",
      "Tokenizing premise_train\n",
      "Tokenizing hypothesis_val\n",
      "Tokenizing premise_val\n",
      "fiction done!\n",
      "\n",
      "Tokenizing hypothesis_train\n",
      "Tokenizing premise_train\n",
      "Tokenizing hypothesis_val\n",
      "Tokenizing premise_val\n",
      "government done!\n",
      "\n",
      "Tokenizing hypothesis_train\n",
      "Tokenizing premise_train\n",
      "Tokenizing hypothesis_val\n",
      "Tokenizing premise_val\n",
      "slate done!\n",
      "\n",
      "Tokenizing hypothesis_train\n",
      "Tokenizing premise_train\n",
      "Tokenizing hypothesis_val\n",
      "Tokenizing premise_val\n",
      "telephone done!\n",
      "\n",
      "Tokenizing hypothesis_train\n",
      "Tokenizing premise_train\n",
      "Tokenizing hypothesis_val\n",
      "Tokenizing premise_val\n",
      "travel done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for genre_choose in ['fiction', 'government', 'slate', 'telephone', 'travel']:\n",
    "    \n",
    "    \n",
    "    all_raw_data_train = data_train.loc[data_train.genre == genre_choose].drop(['genre'],axis=1).reset_index(drop=True)\n",
    "    hypo_train=all_raw_data_train[\"sentence1\"]\n",
    "    prem_train=all_raw_data_train[\"sentence2\"]\n",
    "    label_train=all_raw_data_train[\"label\"]\n",
    "    # read all validation data\n",
    "    data_val = pd.read_csv(path + \"mnli_val.tsv\", sep='\\t')\n",
    "    all_raw_data_val = data_val.loc[data_val.genre == genre_choose].drop(['genre'],axis=1).reset_index(drop=True)\n",
    "    hypo_val=all_raw_data_val[\"sentence1\"]\n",
    "    prem_val=all_raw_data_val[\"sentence2\"]\n",
    "    label_val=all_raw_data_val[\"label\"]\n",
    "    d ={'neutral': 0, 'entailment': 1, 'contradiction': 2}\n",
    "    label_index_train =label_train.map(d, na_action='ignore')\n",
    "    label_index_val =label_val.map(d, na_action='ignore')\n",
    "\n",
    "    tokenizer=spacy.load(\"en_core_web_sm\") # What does this do?\n",
    "    punctuations=string.punctuation\n",
    "\n",
    "    #train set tokens\n",
    "    print (\"Tokenizing hypothesis_train\")\n",
    "    hypo_data_tokens_train, all_hypo_data_tokens_train = tokenize_dataset(hypo_train)\n",
    "    pkl.dump(hypo_data_tokens_train, open(\"hypo_data_tokens_train_{}.p\".format(genre_choose), \"wb\"))\n",
    "    pkl.dump(all_hypo_data_tokens_train, open(\"all_hypo_data_tokens_train_{}.p\".format(genre_choose), \"wb\"))\n",
    "\n",
    "\n",
    "    print (\"Tokenizing premise_train\")\n",
    "    prem_data_tokens_train, all_prem_data_tokens_train = tokenize_dataset(prem_train)\n",
    "    pkl.dump(prem_data_tokens_train, open(\"prem_data_tokens_train_{}.p\".format(genre_choose), \"wb\"))\n",
    "    pkl.dump(all_prem_data_tokens_train, open(\"all_prem_data_tokens_train_{}.p\".format(genre_choose), \"wb\"))\n",
    "\n",
    "\n",
    "    # val set tokens\n",
    "    print (\"Tokenizing hypothesis_val\")\n",
    "    hypo_data_tokens_val, all_hypo_data_tokens_val = tokenize_dataset(hypo_val)\n",
    "    pkl.dump(hypo_data_tokens_val, open(\"hypo_data_tokens_val_{}.p\".format(genre_choose), \"wb\"))\n",
    "    pkl.dump(all_hypo_data_tokens_val, open(\"all_hypo_data_tokens_val_{}.p\".format(genre_choose), \"wb\"))\n",
    "\n",
    "\n",
    "    print (\"Tokenizing premise_val\")\n",
    "    prem_data_tokens_val, all_prem_data_tokens_val = tokenize_dataset(prem_val)\n",
    "    pkl.dump(prem_data_tokens_val, open(\"prem_data_tokens_val_{}.p\".format(genre_choose), \"wb\"))\n",
    "    pkl.dump(all_prem_data_tokens_val, open(\"all_prem_data_tokens_val_{}.p\".format(genre_choose), \"wb\"))\n",
    "\n",
    "    pkl.dump(label_index_train, open(\"label_index_train_{}.p\".format(genre_choose), \"wb\"))\n",
    "    pkl.dump(label_index_val, open(\"label_index_val_{}.p\".format(genre_choose), \"wb\"))\n",
    "    \n",
    "    print('{} done!\\n'.format(genre_choose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294135"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_hypo_data_tokens_train) # non-unqiue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14057"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_hypo_data_tokens_val) # non-unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo_data_tokens_train = pkl.load(open(\"./data_after_process/hypo_data_tokens_train.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'young',\n",
       " 'girl',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pink',\n",
       " 'shirt',\n",
       " 'sitting',\n",
       " 'on',\n",
       " 'a',\n",
       " 'dock',\n",
       " 'viewing',\n",
       " 'a',\n",
       " 'body',\n",
       " 'of',\n",
       " 'water']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypo_data_tokens_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000 # How to select this size? A hyperparameter?\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size)) # 只找出现次数前max vocab size多的tokens\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id_hypo, id2token_hypo = build_vocab(all_hypo_data_tokens_train)\n",
    "token2id_prem, id2token_prem = build_vocab(all_prem_data_tokens_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002\n"
     ]
    }
   ],
   "source": [
    "print(len(token2id_hypo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002\n"
     ]
    }
   ],
   "source": [
    "print(len(id2token_hypo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 224 ; token reading\n",
      "Token reading; token id 224\n"
     ]
    }
   ],
   "source": [
    "random_token_id = random.randint(0, len(id2token_hypo)-1)\n",
    "random_token = id2token_hypo[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token_hypo[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id_hypo[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset hypothese size is 100000\n",
      "Train dataset premise size is 100000\n",
      "Val dataset hypothesis size is 1000\n",
      "Val dataset premise size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_hypo(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id_hypo[token] if token in token2id_hypo else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "def token2index_prem(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id_prem[token] if token in token2id_prem else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "hypo_data_indices_train = token2index_hypo(hypo_data_tokens_train)\n",
    "prem_data_indices_train = token2index_prem(prem_data_tokens_train)\n",
    "hypo_data_indices_val = token2index_hypo(hypo_data_tokens_val)\n",
    "prem_data_indices_val= token2index_prem(prem_data_tokens_val)\n",
    "# double checking\n",
    "print (\"Train dataset hypothese size is {}\".format(len(hypo_data_indices_train)))\n",
    "print (\"Train dataset premise size is {}\".format(len(prem_data_indices_train)))\n",
    "print (\"Val dataset hypothesis size is {}\".format(len(hypo_data_indices_val)))\n",
    "print (\"Val dataset premise size is {}\".format(len(prem_data_indices_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'man', 'gathers', 'kite', 'string', 'on', 'the', 'beach']\n",
      "[2, 7, 1551, 1678, 1110, 6, 4, 82]\n",
      "['a', 'man', 'is', 'playing', 'at', 'the', 'seaside', 'with', 'a', 'big', 'kite']\n",
      "[2, 6, 4, 18, 14, 3, 6564, 16, 2, 262, 1190]\n"
     ]
    }
   ],
   "source": [
    "rand_training_example_id = random.randint(0, len(prem_data_tokens_train ) - 1)\n",
    "print (hypo_data_tokens_train[rand_training_example_id])\n",
    "print(hypo_data_indices_train[rand_training_example_id])\n",
    "print (prem_data_tokens_train[rand_training_example_id])\n",
    "print(prem_data_indices_train[rand_training_example_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence size in hypothesis: 12.94135\n",
      "Average sentence size in premise: 7.43372\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print(\"Average sentence size in hypothesis: \"+str(mean([len(x) for x in hypo_data_tokens_train])))\n",
    "print(\"Average sentence size in premise: \"+str(mean([len(x) for x in prem_data_tokens_train])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=[len(x) for x in hypo_data_tokens_train]\n",
    "t2=[len(x) for x in prem_data_tokens_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x144e49f98>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.distplot(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13ab87ac8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 20\n",
    "# max_sentence_length_prem = 20 # Why do we set differen max sentence length for hypo and prem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hypo_list, prem_list, target_list, max_sentence_length): # modify this parameter, only constant takes UPPER CASE name\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.hypo_list = hypo_list\n",
    "        self.prem_list = prem_list\n",
    "        self.target_list = target_list\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        assert (len(self.hypo_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hypo_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_hypo_idx = self.hypo_list[key][:self.max_sentence_length]\n",
    "        token_prem_idx = self.prem_list[key][:self.max_sentence_length]\n",
    "        label = self.target_list[key]\n",
    "        return [token_hypo_idx, len(token_hypo_idx), token_prem_idx, len(token_prem_idx), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NewsGroupDataset(hypo_data_indices_train, prem_data_indices_train, label_index, max_sentence_length)\n",
    "val_dataset = NewsGroupDataset(hypo_data_indices_val, prem_data_indices_val, label_index_val,max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence length in training dataset:\n",
      "hypo: 16 prem: 9\n",
      "hypo: 20 prem: 7\n",
      "hypo: 9 prem: 9\n",
      "hypo: 14 prem: 9\n",
      "hypo: 9 prem: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Example sentence length in training dataset:\")\n",
    "for i in range(5):\n",
    "    print('hypo:',train_dataset[i][1],'prem:',train_dataset[i][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example in training dataset:\n",
      "x \n",
      "[2, 22, 29, 3, 2, 83, 19, 27, 6, 2, 635, 1660, 2, 298, 9, 44]\n",
      "[2, 33, 20, 64, 3, 907, 118, 3, 46];\n",
      "y 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Example in training dataset:\")\n",
    "print(\"x \\n{}\\n{};\\ny {}\".format(train_dataset[0][0], train_dataset[0][2],train_dataset[0][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    hypo_list = []\n",
    "    len_hypo_list = []\n",
    "    prem_list = []\n",
    "    len_prem_list = []\n",
    "    label_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch)\n",
    "    #batch[0][0] = batch[0][0][:max_sentence_length_prem]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        len_hypo_list.append(datum[1])\n",
    "        len_prem_list.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        # hypo\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,max_sentence_length-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        hypo_list.append(padded_vec)\n",
    "        # prem\n",
    "        padded_vec = np.pad(np.array(datum[2]), \n",
    "                                pad_width=((0,max_sentence_length-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        prem_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(hypo_list)), torch.LongTensor(len_hypo_list), torch.from_numpy(np.array(prem_list)), torch.LongTensor(len_prem_list),torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  18,  120,    7,  ...,  642,   78,  110],\n",
      "        [   2,    7,    5,  ...,    0,    0,    0],\n",
      "        [2117,  182, 9786,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  48,   13, 4018,  ...,    0,    0,    0],\n",
      "        [ 118,   79,    5,  ...,    0,    0,    0],\n",
      "        [   2,   22,   29,  ...,    2,   26,   19]])\n",
      "tensor([[ 23,   5,  13,  ...,   0,   0,   0],\n",
      "        [  2,  71,   4,  ...,   0,   0,   0],\n",
      "        [  9,   5, 444,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [901,   5,   7,  ...,   0,   0,   0],\n",
      "        [ 48,  68,  18,  ...,   0,   0,   0],\n",
      "        [  2,  20,   4,  ...,   0,   0,   0]])\n",
      "tensor([1, 1, 1, 2, 2, 0, 2, 0, 1, 1, 2, 0, 2, 1, 0, 0, 1, 2, 1, 0, 0, 0, 1, 1,\n",
      "        1, 0, 2, 0, 2, 2, 1, 1, 2, 2, 1, 2, 0, 0, 1, 2, 1, 1, 0, 1, 2, 2, 1, 1,\n",
      "        1, 0, 0, 2, 0, 0, 2, 1, 1, 0, 1, 0, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 0,\n",
      "        1, 0, 1, 1, 0, 2, 0, 0, 0, 1, 2, 2, 2, 0, 1, 1, 2, 0, 1, 2, 1, 1, 2, 0,\n",
      "        0, 1, 1, 1, 0, 2, 1, 2, 1, 2, 2, 0, 2, 1, 2, 2, 0, 1, 1, 0, 2, 2, 0, 2,\n",
      "        0, 2, 2, 2, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "for i, (hypo, len_hypo, prem, len_prem, labels) in enumerate(train_loader):\n",
    "    pprint(hypo)\n",
    "    pprint(prem)\n",
    "    pprint(labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "################################################\n",
    "# This is a concatenate model NN\n",
    "# 64-32\n",
    "################################################\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # 1. Embedding\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        \n",
    "        # 2. an affine operation: y=Wx+b\n",
    "        self.hidden_1= nn.Linear(2*emb_dim,64)\n",
    "        self.hidden_2=nn.Linear(64,32)\n",
    "        self.output = nn.Linear(32,3)\n",
    "    \n",
    "    def forward(self, data_hypo, length_hypo, data_prem, length_prem):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        # word embedding\n",
    "        out_hypo = self.embed(data_hypo)\n",
    "        out_prem = self.embed(data_prem)\n",
    "        # combine to sentence \n",
    "        out_prem = torch.sum(out_prem, dim=1)\n",
    "        out_hypo = torch.sum(out_hypo, dim=1)\n",
    "        out_prem /= length_prem.view(length_prem.size()[0], 1).expand_as(out_prem).float()\n",
    "        out_hypo /= length_hypo.view(length_hypo.size()[0], 1).expand_as(out_hypo).float()\n",
    "\n",
    "        # now use concat, add or product\n",
    "        out=torch.cat((out_hypo,out_prem),1)\n",
    "#         out = torch.add(out_hypo, out_prem)\n",
    "\n",
    "        out = self.hidden_1(out.float())\n",
    "        out = F.relu(out)\n",
    "        out = self.hidden_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 1000\n",
    "model = BagOfWords(len(id2token_hypo), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BagOfWords(\n",
      "  (embed): Embedding(10002, 1000, padding_idx=0)\n",
      "  (hidden_1): Linear(in_features=2000, out_features=64, bias=True)\n",
      "  (hidden_2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (output): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10002, 1000])\n",
      "torch.Size([64, 2000])\n",
      "torch.Size([64])\n",
      "torch.Size([32, 64])\n",
      "torch.Size([32])\n",
      "torch.Size([3, 32])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for x in model.parameters():\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear anealling lr and validation loss only sections below are changed\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "# for optimizer: add arg \"weight_decay\" for regularizer (float, optional) – weight decay (L2 penalty) (default: 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# linear annealling of lr\n",
    "# reference:https://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.LambdaLR\n",
    "# Sets the learning rate to the initial lr times a given function.\n",
    "# A function which computes a multiplicative factor given an integer parameter epoch. \n",
    "annealing_lr = lambda epoch: 0.95 ** epoch # decrease with the epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=annealing_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.005\n",
      "lr 0.00475\n",
      "lr 0.0045125\n",
      "lr 0.004286875\n",
      "lr 0.00407253125\n",
      "lr 0.003868904687499999\n",
      "lr 0.003675459453124999\n",
      "lr 0.003491686480468749\n",
      "lr 0.0033171021564453113\n",
      "lr 0.0031512470486230455\n"
     ]
    }
   ],
   "source": [
    "# This shows how lr change each epoch\n",
    "# !!!NOTE!!! Rerun the last cell before training, so a scheduler can be re-initialize\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print('lr',optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # no loss curve\n",
    "# num_epochs = 10 # number epoch to train\n",
    "\n",
    "# # Function for testing the model\n",
    "# def test_model(data_loader, model):\n",
    "#     \"\"\"\n",
    "#     Help function that tests the model's performance on a dataset\n",
    "#     @param: loader - data loader for the dataset to test against\n",
    "#     \"\"\"\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     model.eval()\n",
    "#     for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(train_loader):\n",
    "#         outputs = F.softmax(model(data_hypo, lengths_hypo, data_prem, lengths_prem), dim=1)\n",
    "#         predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "#         total += labels.size(0)\n",
    "#         correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "#     return (100 * correct / total)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(train_loader):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass\n",
    "\n",
    "#         outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem)\n",
    "        \n",
    "#         # Compute loss\n",
    "#         loss = criterion(outputs,labels)\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         # validate every 100 iterations\n",
    "\n",
    "#         if i > 0 and i % 100 == 0:\n",
    "#             # validate\n",
    "#             val_acc = test_model(val_loader, model)\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "#                        epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.01\n",
      "lr 0.005\n",
      "lr 0.003333333333333333\n",
      "lr 0.0025\n",
      "lr 0.002\n"
     ]
    }
   ],
   "source": [
    "# record loss of every epoch\n",
    "losses = []\n",
    "\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(data_loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(data_loader):\n",
    "        outputs = F.softmax(model(data_hypo, lengths_hypo, data_prem, lengths_prem), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print('lr',optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "\n",
    "        outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0:\n",
    "#         if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "        ###?\n",
    "    scheduler.step()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14eebec18>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following part is a training section sample with updating lr and record both train and val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.005\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-7d72304b0fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# training acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mrunning_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-7d72304b0fac>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(data_loader, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_hypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_hypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_prem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_prem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_hypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_hypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_prem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths_prem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NLP_NYU/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-276fda7489da>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_hypo, length_hypo, data_prem, length_prem)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mout_prem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_prem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mout_hypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_hypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mout_prem\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlength_prem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_prem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_prem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mout_hypo\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlength_hypo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_hypo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_hypo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# record loss and accuracy of every epoch\n",
    "losses = []\n",
    "val_losses = []\n",
    "acces = []\n",
    "val_acces = []\n",
    "\n",
    "\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(data_loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \n",
    "    return:\n",
    "    accuracy, loss\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(data_loader):\n",
    "        outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        # Compute acc\n",
    "        outputs_softmax = F.softmax(outputs, dim=1)\n",
    "        predicted = outputs_softmax.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), loss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print('lr',optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    running_val_acc = 0.0\n",
    "    \n",
    "    for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(train_loader):\n",
    "        print(i)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # training acc\n",
    "        train_acc, _ = test_model(train_loader, model)\n",
    "        running_acc += train_acc\n",
    "\n",
    "        # validation would be implement in each time (batch)\n",
    "        val_acc, val_loss = test_model(val_loader, model)\n",
    "        # print val_acc every 100 iterations\n",
    "        if i>0 and i%100==0:\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "        running_val_loss += val_loss.item()\n",
    "        running_val_acc += val_acc\n",
    "     \n",
    "    \n",
    "    # Actions after one epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_val_loss = running_val_loss/ len(train_loader)\n",
    "    epoch_acc = running_acc / len(train_loader)\n",
    "    epoch_val_acc = running_val_acc/ len(train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    acces.append(epoch_acc)\n",
    "    val_acces.append(epoch_val_acc)\n",
    "    # after one epoch, do change the learning rate\n",
    "    scheduler.step()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.plot(val_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
