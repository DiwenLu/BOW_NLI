{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validataion data are all needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hypo_list, prem_list, target_list, max_sentence_length):\n",
    "        \"\"\"\n",
    "        @param hypo_list: list of hypo tokens\n",
    "        @param prem_list: list of prem tokens\n",
    "        @param target_list: list of newsgroup targets\n",
    "        @param max_sentence_length: fixed length of all sentence\n",
    "\n",
    "        \"\"\"\n",
    "        self.hypo_list = hypo_list\n",
    "        self.prem_list = prem_list\n",
    "        self.target_list = target_list\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        assert (len(self.hypo_list) == len(self.target_list))\n",
    "        assert (len(self.prem_list) == len(self.target_list))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hypo_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "            Triggered when you call dataset[i]\n",
    "            \"\"\"\n",
    "\n",
    "        token_hypo_idx = self.hypo_list[key][:self.max_sentence_length]\n",
    "        token_prem_idx = self.prem_list[key][:self.max_sentence_length]\n",
    "        label = self.target_list[key]\n",
    "        return [token_hypo_idx, len(token_hypo_idx), token_prem_idx, len(token_prem_idx), label]\n",
    "\n",
    "\n",
    "class NeuralNetworkPytorch(nn.Module):\n",
    "    \"\"\"\n",
    "    NeuralNetwork classification model\n",
    "    Model would change according to interaction_type\n",
    "\n",
    "    1st hidden layer: 90 neurons\n",
    "    2nd hidden layer: 90 neurons\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, n_out, interaction_type):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary.\n",
    "        @param emb_dim: size of the word embedding\n",
    "        @param n_out: size of the class.\n",
    "        \"\"\"\n",
    "        super(NeuralNetworkPytorch, self).__init__()\n",
    "\n",
    "        # 1. Embedding\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        # 2. an affine operation: y=Wx+b\n",
    "        # double embedding dimension if we concat hypo's and prem's embedding\n",
    "        if interaction_type == 'concat':\n",
    "            emb_dim = 2 * emb_dim\n",
    "        self.hidden_1= nn.Linear(emb_dim,90)\n",
    "        self.hidden_2=nn.Linear(90, 90)\n",
    "        self.output = nn.Linear(90, n_out)\n",
    "\n",
    "    def forward(self, data_hypo, length_hypo, data_prem, length_prem, interaction_type):\n",
    "        \"\"\"\n",
    "            @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a\n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "            @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            @param data_prem: matrix of size (batch_size, max_sentence_length).\n",
    "            @param length_hypo: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "                length of each sentences in the data_prem.\n",
    "            @param interaction_type: [sum. hadamart, concat]\n",
    "            \"\"\"\n",
    "        # word embedding\n",
    "        out_hypo = self.embed(data_hypo)\n",
    "        out_prem = self.embed(data_prem)\n",
    "        # combine to sentence\n",
    "        out_prem = torch.sum(out_prem, dim=1)\n",
    "        out_hypo = torch.sum(out_hypo, dim=1)\n",
    "        out_prem /= length_prem.view(length_prem.size()[0], 1).expand_as(out_prem).float()\n",
    "        out_hypo /= length_hypo.view(length_hypo.size()[0], 1).expand_as(out_hypo).float()\n",
    "\n",
    "        # interaction\n",
    "        # 1. sum\n",
    "        # 2. Hadamard product\n",
    "        # 3. concat (This will change embedding dimension, 2 times as many as before)\n",
    "        if interaction_type == 'concat':\n",
    "            out = torch.cat((out_hypo,out_prem),1)\n",
    "        if interaction_type == 'sum':\n",
    "            out = torch.add(out_hypo, out_prem)\n",
    "        if interaction_type == 'hadamart':\n",
    "            out = out_hypo * out_prem\n",
    "\n",
    "        out = self.hidden_1(out.float())\n",
    "        out = F.relu(out)\n",
    "        out = self.hidden_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    hypo_list = []\n",
    "    len_hypo_list = []\n",
    "    prem_list = []\n",
    "    len_prem_list = []\n",
    "    label_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch)\n",
    "    #batch[0][0] = batch[0][0][:max_sentence_length_prem]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        len_hypo_list.append(datum[1])\n",
    "        len_prem_list.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        # hypo\n",
    "        padded_vec = np.pad(np.array(datum[0]), pad_width=((0,max_sentence_length-datum[1])), mode=\"constant\", constant_values=0)\n",
    "        hypo_list.append(padded_vec)\n",
    "        # prem\n",
    "        padded_vec = np.pad(np.array(datum[2]), pad_width=((0,max_sentence_length-datum[3])), mode=\"constant\", constant_values=0)\n",
    "        prem_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(hypo_list)), torch.LongTensor(len_hypo_list), torch.from_numpy(np.array(prem_list)), torch.LongTensor(len_prem_list),torch.LongTensor(label_list)]\n",
    "\n",
    "def test_model(data_loader, model, interaction_type):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "\n",
    "    return:\n",
    "    accuracy, loss\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(data_loader):\n",
    "        outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem, interaction_type)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs,labels)\n",
    "        # Compute acc\n",
    "        outputs_softmax = F.softmax(outputs, dim=1)\n",
    "        predicted = outputs_softmax.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), loss.item()\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "def build_vocab(hypo_tokens, prem_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "\n",
    "    hypo_token_counter = Counter(hypo_tokens)\n",
    "    prem_token_counter = Counter(prem_tokens)\n",
    "\n",
    "    all_tokens_counter = hypo_token_counter + prem_token_counter\n",
    "\n",
    "    vocab, count = zip(*all_tokens_counter.most_common(max_vocab_size))\n",
    "\n",
    "    # print(all_tokens_counter.most_common(MAX_VOCAB_SIZE))\n",
    "\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2, 2 + len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction\n",
      "travel\n",
      "government\n",
      "slate\n",
      "telephone\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "# this is the paramter of the best model\n",
    "BATCH_SIZE = 1024\n",
    "max_sentence_length = 20\n",
    "max_vocab_size = 10000\n",
    "emb_dim = 100\n",
    "interaction_type = 'concat'\n",
    "best_model_save = './best_nn_model.pth.tar'\n",
    "\n",
    "genres = ['fiction', 'travel', 'government', 'slate', 'telephone']\n",
    "folder = './multi_data_after_process/'\n",
    "all_hypo_data_tokens_train = pkl.load(open(folder+\"all_hypo_data_tokens_train.p\", \"rb\"))\n",
    "all_prem_data_tokens_train = pkl.load(open(folder+\"all_prem_data_tokens_train.p\", \"rb\"))\n",
    "token2id, id2token = build_vocab(all_hypo_data_tokens_train, all_prem_data_tokens_train, max_vocab_size)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mult_genre_val_acc = {}\n",
    "\n",
    "for genre in genres:\n",
    "    print(genre)\n",
    "    result = {}\n",
    "    hypo_data_tokens_val = pkl.load(open(folder+\"hypo_data_tokens_val_{}.p\".format(genre), \"rb\"))\n",
    "    prem_data_tokens_val = pkl.load(open(folder+\"prem_data_tokens_val_{}.p\".format(genre), \"rb\"))\n",
    "    hypo_data_tokens_train = pkl.load(open(folder+\"hypo_data_tokens_train_{}.p\".format(genre), \"rb\"))\n",
    "    prem_data_tokens_train = pkl.load(open(folder+\"prem_data_tokens_train_{}.p\".format(genre), \"rb\"))\n",
    "    label_index_train = pkl.load(open(folder+\"label_index_train_{}.p\".format(genre), \"rb\"))\n",
    "    label_index_val = pkl.load(open(folder+\"label_index_val_{}.p\".format(genre), \"rb\"))\n",
    "    \n",
    "    hypo_data_indices_train = token2index(hypo_data_tokens_train)\n",
    "    prem_data_indices_train = token2index(prem_data_tokens_train)\n",
    "    hypo_data_indices_val = token2index(hypo_data_tokens_val)\n",
    "    prem_data_indices_val= token2index(prem_data_tokens_val)\n",
    "\n",
    "    \n",
    "    train_dataset = NewsGroupDataset(hypo_data_indices_train, prem_data_indices_train, label_index_train, max_sentence_length)\n",
    "    val_dataset = NewsGroupDataset(hypo_data_indices_val, prem_data_indices_val, label_index_val,max_sentence_length)\n",
    "    # seperate dataset into different batch\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, collate_fn=newsgroup_collate_func,shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE,collate_fn=newsgroup_collate_func,shuffle=True)\n",
    "    \n",
    "    # fine tune parameter\n",
    "    learning_rate = 0.005\n",
    "    num_epochs = 20\n",
    "    \n",
    "    #initialize model\n",
    "    model = NeuralNetworkPytorch(len(id2token), emb_dim, len(set(label_index_train)), interaction_type)\n",
    "    checkpoint = torch.load(best_model_save)\n",
    "    # load the pretrained model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # for optimizer: add arg \"weight_decay\" for regularizer (float, optional) â€“ weight decay (L2 penalty) (default: 0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_losses = [] # record training loss after every batch\n",
    "    val_accs = [] # record validation accuracy after every batch\n",
    "    # Save state_dict for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_acc_val = 0.0\n",
    "        running_loss_train = 0.0\n",
    "        for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem, interaction_type)\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs,labels)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ### training\n",
    "            # training loss\n",
    "            train_loss = loss.item()\n",
    "\n",
    "            ### validation would be implement in each time (batch)\n",
    "            val_acc, val_loss = test_model(val_loader, model, interaction_type)\n",
    "            running_acc_val += val_acc\n",
    "            running_loss_train += train_loss\n",
    "            \n",
    "        val_accs.append(running_acc_val/len(train_loader))\n",
    "        train_losses.append(running_loss_train/len(train_loader))\n",
    "    \n",
    "    result = {'train_losses': train_losses, 'val_accs': val_accs}\n",
    "    mult_genre_val_acc[genre]=result\n",
    "\n",
    "pkl.dump(mult_genre_val_acc, open(\"3_3_tuned_NN_val_acc.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction': {'train_losses': [1.2926235496997833,\n",
       "   1.029111698269844,\n",
       "   0.9957960397005081,\n",
       "   0.9677331447601318,\n",
       "   0.9271656423807144,\n",
       "   0.8775793761014938,\n",
       "   0.8227683454751968,\n",
       "   0.7704958319664001,\n",
       "   0.7144752889871597,\n",
       "   0.6497127562761307,\n",
       "   0.5837040692567825,\n",
       "   0.5148798525333405,\n",
       "   0.44916214793920517,\n",
       "   0.3800961449742317,\n",
       "   0.3176043778657913,\n",
       "   0.25886500626802444,\n",
       "   0.20365944132208824,\n",
       "   0.15823186188936234,\n",
       "   0.11797746270895004,\n",
       "   0.08573306165635586],\n",
       "  'val_accs': [42.63819095477387,\n",
       "   42.73869346733668,\n",
       "   43.31658291457287,\n",
       "   43.84422110552764,\n",
       "   43.26633165829146,\n",
       "   44.54773869346734,\n",
       "   44.44723618090452,\n",
       "   43.69346733668342,\n",
       "   43.065326633165824,\n",
       "   44.02010050251256,\n",
       "   44.17085427135679,\n",
       "   44.54773869346734,\n",
       "   44.97487437185929,\n",
       "   44.72361809045226,\n",
       "   44.070351758793976,\n",
       "   44.42211055276382,\n",
       "   44.62311557788944,\n",
       "   45.0251256281407,\n",
       "   45.45226130653266,\n",
       "   45.35175879396985]},\n",
       " 'travel': {'train_losses': [1.2286595106124878,\n",
       "   1.015045389533043,\n",
       "   0.9787865281105042,\n",
       "   0.948004886507988,\n",
       "   0.9000864028930664,\n",
       "   0.8514826893806458,\n",
       "   0.7986704111099243,\n",
       "   0.7465033382177353,\n",
       "   0.6875156313180923,\n",
       "   0.6242335885763168,\n",
       "   0.556549146771431,\n",
       "   0.4854833632707596,\n",
       "   0.4135304465889931,\n",
       "   0.3408612683415413,\n",
       "   0.27356383204460144,\n",
       "   0.21248704567551613,\n",
       "   0.1591700091958046,\n",
       "   0.11414149031043053,\n",
       "   0.07896333187818527,\n",
       "   0.05365034472197294],\n",
       "  'val_accs': [44.93380855397149,\n",
       "   45.34114052953157,\n",
       "   45.79938900203666,\n",
       "   46.05397148676171,\n",
       "   47.17413441955193,\n",
       "   47.657841140529534,\n",
       "   48.70162932790224,\n",
       "   49.41446028513238,\n",
       "   49.31262729124236,\n",
       "   49.61812627291242,\n",
       "   49.71995926680245,\n",
       "   49.59266802443992,\n",
       "   48.95621181262729,\n",
       "   48.548879837067204,\n",
       "   48.676171079429736,\n",
       "   48.34521384928716,\n",
       "   48.06517311608961,\n",
       "   48.06517311608961,\n",
       "   48.14154786150712,\n",
       "   48.319755600814666]},\n",
       " 'government': {'train_losses': [1.2282346189022064,\n",
       "   1.009506180882454,\n",
       "   0.9720954746007919,\n",
       "   0.938417598605156,\n",
       "   0.8956824243068695,\n",
       "   0.8568818867206573,\n",
       "   0.8129158914089203,\n",
       "   0.765498086810112,\n",
       "   0.7151751071214676,\n",
       "   0.6595176309347153,\n",
       "   0.5977649688720703,\n",
       "   0.5310218334197998,\n",
       "   0.4630355164408684,\n",
       "   0.3959457278251648,\n",
       "   0.3265378698706627,\n",
       "   0.26417283713817596,\n",
       "   0.206246055662632,\n",
       "   0.15749752521514893,\n",
       "   0.11622178554534912,\n",
       "   0.08321931026875973],\n",
       "  'val_accs': [44.51279527559055,\n",
       "   45.54625984251969,\n",
       "   45.620078740157474,\n",
       "   47.36712598425197,\n",
       "   48.105314960629926,\n",
       "   48.449803149606296,\n",
       "   48.03149606299213,\n",
       "   47.73622047244095,\n",
       "   47.85925196850394,\n",
       "   48.27755905511811,\n",
       "   48.57283464566929,\n",
       "   48.17913385826772,\n",
       "   47.88385826771653,\n",
       "   48.105314960629926,\n",
       "   47.66240157480314,\n",
       "   47.78543307086614,\n",
       "   48.12992125984252,\n",
       "   48.277559055118104,\n",
       "   48.17913385826772,\n",
       "   48.35137795275591]},\n",
       " 'slate': {'train_losses': [1.3395064175128937,\n",
       "   1.0659590661525726,\n",
       "   1.0232435762882233,\n",
       "   1.0015406608581543,\n",
       "   0.9698359817266464,\n",
       "   0.9259137213230133,\n",
       "   0.8760768175125122,\n",
       "   0.8253876864910126,\n",
       "   0.7724633812904358,\n",
       "   0.7125882655382156,\n",
       "   0.6454039365053177,\n",
       "   0.577027827501297,\n",
       "   0.5049682110548019,\n",
       "   0.4319423586130142,\n",
       "   0.36010559648275375,\n",
       "   0.291248582303524,\n",
       "   0.2276126593351364,\n",
       "   0.17174994200468063,\n",
       "   0.12546205893158913,\n",
       "   0.08904301933944225],\n",
       "  'val_accs': [37.77445109780439,\n",
       "   36.05289421157685,\n",
       "   36.47704590818363,\n",
       "   37.37524950099801,\n",
       "   38.19860279441118,\n",
       "   39.09680638722555,\n",
       "   39.271457085828345,\n",
       "   39.97005988023952,\n",
       "   39.895209580838326,\n",
       "   40.26946107784431,\n",
       "   40.144710578842314,\n",
       "   39.221556886227546,\n",
       "   39.64570858283433,\n",
       "   40.24451097804391,\n",
       "   39.02195608782435,\n",
       "   38.92215568862275,\n",
       "   38.697604790419156,\n",
       "   38.647704590818364,\n",
       "   38.647704590818364,\n",
       "   37.949101796407184]},\n",
       " 'telephone': {'train_losses': [1.3079421997070313,\n",
       "   1.0550626039505004,\n",
       "   1.0332629919052123,\n",
       "   1.0137333393096923,\n",
       "   0.9705589771270752,\n",
       "   0.9253279209136963,\n",
       "   0.8952713012695312,\n",
       "   0.8505964636802673,\n",
       "   0.7943232536315918,\n",
       "   0.7480174660682678,\n",
       "   0.6907254219055176,\n",
       "   0.6390910744667053,\n",
       "   0.5914398550987243,\n",
       "   0.5159463524818421,\n",
       "   0.452523285150528,\n",
       "   0.3782316505908966,\n",
       "   0.33022710084915163,\n",
       "   0.27853127717971804,\n",
       "   0.22494176030158997,\n",
       "   0.1866736441850662],\n",
       "  'val_accs': [42.80597014925373,\n",
       "   41.25373134328358,\n",
       "   42.5273631840796,\n",
       "   41.61194029850746,\n",
       "   42.82587064676617,\n",
       "   43.86069651741293,\n",
       "   44.895522388059696,\n",
       "   46.189054726368155,\n",
       "   46.26865671641791,\n",
       "   47.08457711442786,\n",
       "   46.58706467661692,\n",
       "   46.58706467661691,\n",
       "   45.99004975124378,\n",
       "   45.492537313432834,\n",
       "   45.57213930348259,\n",
       "   45.73134328358209,\n",
       "   45.49253731343284,\n",
       "   45.134328358208954,\n",
       "   45.07462686567165,\n",
       "   45.43283582089552]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult_genre_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
