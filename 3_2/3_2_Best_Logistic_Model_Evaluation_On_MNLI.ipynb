{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluating on MultiNLI (Best Logistic Regression Model)\n",
    "**ONLY VALIDATION DATA ARE USED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle as pkl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hypo_list, prem_list, target_list, max_sentence_length):\n",
    "        \"\"\"\n",
    "        @param hypo_list: list of hypo tokens\n",
    "        @param prem_list: list of prem tokens\n",
    "        @param target_list: list of newsgroup targets\n",
    "        @param max_sentence_length: fixed length of all sentence\n",
    "\n",
    "        \"\"\"\n",
    "        self.hypo_list = hypo_list\n",
    "        self.prem_list = prem_list\n",
    "        self.target_list = target_list\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        assert (len(self.hypo_list) == len(self.target_list))\n",
    "        assert (len(self.prem_list) == len(self.target_list))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hypo_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "            Triggered when you call dataset[i]\n",
    "            \"\"\"\n",
    "\n",
    "        token_hypo_idx = self.hypo_list[key][:self.max_sentence_length]\n",
    "        token_prem_idx = self.prem_list[key][:self.max_sentence_length]\n",
    "        label = self.target_list[key]\n",
    "        return [token_hypo_idx, len(token_hypo_idx), token_prem_idx, len(token_prem_idx), label]\n",
    "\n",
    "\n",
    "# Function for testing the model\n",
    "class NeuralNetworkPytorch(nn.Module):\n",
    "    \"\"\"\n",
    "    NeuralNetwork classification model\n",
    "    Model would change according to interaction_type\n",
    "\n",
    "    1st hidden layer: 90 neurons\n",
    "    2nd hidden layer: 90 neurons\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, n_out, interaction_type):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary.\n",
    "        @param emb_dim: size of the word embedding\n",
    "        @param n_out: size of the class.\n",
    "        \"\"\"\n",
    "        super(NeuralNetworkPytorch, self).__init__()\n",
    "\n",
    "        # 1. Embedding\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        # 2. an affine operation: y=Wx+b\n",
    "        # double embedding dimension if we concat hypo's and prem's embedding\n",
    "        if interaction_type == 'concat':\n",
    "            emb_dim = 2 * emb_dim\n",
    "        self.hidden_1= nn.Linear(emb_dim,90)\n",
    "        self.hidden_2=nn.Linear(90, 90)\n",
    "        self.output = nn.Linear(90, n_out)\n",
    "\n",
    "    def forward(self, data_hypo, length_hypo, data_prem, length_prem, interaction_type):\n",
    "        \"\"\"\n",
    "            @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a\n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "            @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "            @param data_prem: matrix of size (batch_size, max_sentence_length).\n",
    "            @param length_hypo: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "                length of each sentences in the data_prem.\n",
    "            @param interaction_type: [sum. hadamart, concat]\n",
    "            \"\"\"\n",
    "        # word embedding\n",
    "        out_hypo = self.embed(data_hypo)\n",
    "        out_prem = self.embed(data_prem)\n",
    "        # combine to sentence\n",
    "        out_prem = torch.sum(out_prem, dim=1)\n",
    "        out_hypo = torch.sum(out_hypo, dim=1)\n",
    "        out_prem /= length_prem.view(length_prem.size()[0], 1).expand_as(out_prem).float()\n",
    "        out_hypo /= length_hypo.view(length_hypo.size()[0], 1).expand_as(out_hypo).float()\n",
    "\n",
    "        # interaction\n",
    "        # 1. sum\n",
    "        # 2. Hadamard product\n",
    "        # 3. concat (This will change embedding dimension, 2 times as many as before)\n",
    "        if interaction_type == 'concat':\n",
    "            out = torch.cat((out_hypo,out_prem),1)\n",
    "        if interaction_type == 'sum':\n",
    "            out = torch.add(out_hypo, out_prem)\n",
    "        if interaction_type == 'hadamart':\n",
    "            out = out_hypo * out_prem\n",
    "\n",
    "        out = self.hidden_1(out.float())\n",
    "        out = F.relu(out)\n",
    "        out = self.hidden_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "class LogisticRegressionPyTorch(nn.Module):\n",
    "    \"\"\"\n",
    "    Logistic regression classification model\n",
    "    Model would change according to interaction_type\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, n_out, interaction_type):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary.\n",
    "        @param emb_dim: size of the word embedding.\n",
    "        @param n_out: size of the class.\n",
    "        \"\"\"\n",
    "        super(LogisticRegressionPyTorch, self).__init__()\n",
    "\n",
    "        # 1. Embedding\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        # 2. Logistic Regression\n",
    "        # double embedding dimension if we concat hypo's and prem's embedding\n",
    "        if interaction_type == 'concat':\n",
    "            emb_dim *= 2\n",
    "        self.linear = nn.Linear(emb_dim, n_out)\n",
    "\n",
    "    def forward(self, data_hypo, length_hypo, data_prem, length_prem, interaction_type):\n",
    "        \"\"\"\n",
    "        @param data_hypo: matrix of size (batch_size, max_sentence_length).\n",
    "        @param length_hypo: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data_hypo.\n",
    "        @param data_prem: matrix of size (batch_size, max_sentence_length).\n",
    "        @param length_hypo: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data_prem.\n",
    "        @param interaction_type: [sum. hadamart, concat]\n",
    "        \"\"\"\n",
    "        out_hypo = self.embed(data_hypo)\n",
    "        out_prem = self.embed(data_prem)\n",
    "\n",
    "        out_hypo = torch.sum(out_hypo, dim=1)\n",
    "        out_prem = torch.sum(out_prem, dim=1)\n",
    "\n",
    "        out_hypo /= length_hypo.view(length_hypo.size()[0],1).expand_as(out_hypo).float()\n",
    "        out_prem /= length_prem.view(length_prem.size()[0],1).expand_as(out_prem).float()\n",
    "\n",
    "        # interaction\n",
    "        # 1. sum\n",
    "        # 2. Hadamard product\n",
    "        # 3. concat (This will change embedding dimension, 2 times as many as before)\n",
    "        if interaction_type == 'concat':\n",
    "            out = torch.cat((out_hypo,out_prem),1)\n",
    "        if interaction_type == 'sum':\n",
    "            out = torch.add(out_hypo,out_prem)\n",
    "        if interaction_type == 'hadamard':\n",
    "            out = out_hypo * out_prem\n",
    "\n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out    \n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    hypo_list = []\n",
    "    len_hypo_list = []\n",
    "    prem_list = []\n",
    "    len_prem_list = []\n",
    "    label_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch)\n",
    "    #batch[0][0] = batch[0][0][:max_sentence_length_prem]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        len_hypo_list.append(datum[1])\n",
    "        len_prem_list.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        # hypo\n",
    "        padded_vec = np.pad(np.array(datum[0]), pad_width=((0,max_sentence_length-datum[1])), mode=\"constant\", constant_values=0)\n",
    "        hypo_list.append(padded_vec)\n",
    "        # prem\n",
    "        padded_vec = np.pad(np.array(datum[2]), pad_width=((0,max_sentence_length-datum[3])), mode=\"constant\", constant_values=0)\n",
    "        prem_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(hypo_list)), torch.LongTensor(len_hypo_list), torch.from_numpy(np.array(prem_list)), torch.LongTensor(len_prem_list),torch.LongTensor(label_list)]\n",
    "\n",
    "def test_model(data_loader, model, interaction_type):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "\n",
    "    return:\n",
    "    accuracy, loss\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    for i, (data_hypo, lengths_hypo, data_prem, lengths_prem, labels) in enumerate(data_loader):\n",
    "        outputs = model(data_hypo, lengths_hypo, data_prem, lengths_prem, interaction_type)\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # compute acc\n",
    "        outputs_softmax = F.softmax(outputs, dim=1)\n",
    "        predicted = outputs_softmax.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total), loss.item()\n",
    "\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "def build_vocab(hypo_tokens, prem_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "\n",
    "    hypo_token_counter = Counter(hypo_tokens)\n",
    "    prem_token_counter = Counter(prem_tokens)\n",
    "\n",
    "    all_tokens_counter = hypo_token_counter + prem_token_counter\n",
    "\n",
    "    vocab, count = zip(*all_tokens_counter.most_common(max_vocab_size))\n",
    "\n",
    "    # print(all_tokens_counter.most_common(MAX_VOCAB_SIZE))\n",
    "\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2, 2 + len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "max_sentence_length = 20\n",
    "# this is the paramter of the best nn model\n",
    "max_vocab_size = 10000\n",
    "BATCH_SIZE = 1024\n",
    "emb_dim = 50\n",
    "interaction_type = 'hadamard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating on fiction genre...\n",
      "The best logistic model's validation accuracy on fiction genre is 41.91.\n",
      "Validating on travel genre...\n",
      "The best logistic model's validation accuracy on travel genre is 37.88.\n",
      "Validating on government genre...\n",
      "The best logistic model's validation accuracy on government genre is 36.12.\n",
      "Validating on slate genre...\n",
      "The best logistic model's validation accuracy on slate genre is 37.62.\n",
      "Validating on telephone genre...\n",
      "The best logistic model's validation accuracy on telephone genre is 40.3.\n"
     ]
    }
   ],
   "source": [
    "genres = ['fiction', 'travel', 'government', 'slate', 'telephone']\n",
    "folder = os.getcwd() + '/../all_data_pickle/'\n",
    "all_hypo_data_tokens_train = pkl.load(open(folder+\"all_hypo_data_tokens_train.p\", \"rb\"))\n",
    "all_prem_data_tokens_train = pkl.load(open(folder+\"all_prem_data_tokens_train.p\", \"rb\"))\n",
    "token2id, id2token = build_vocab(all_hypo_data_tokens_train, all_prem_data_tokens_train, max_vocab_size)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mult_genre_val_acc = {}\n",
    "\n",
    "for genre in genres:\n",
    "    hypo_data_tokens_val = pkl.load(open(folder+\"hypo_data_tokens_val_{}.p\".format(genre), \"rb\"))\n",
    "    prem_data_tokens_val = pkl.load(open(folder+\"prem_data_tokens_val_{}.p\".format(genre), \"rb\"))\n",
    "    label_index_val = pkl.load(open(folder+\"label_index_val_{}.p\".format(genre), \"rb\"))\n",
    "    hypo_data_indices_val = token2index(hypo_data_tokens_val)\n",
    "    prem_data_indices_val= token2index(prem_data_tokens_val)\n",
    "    val_dataset = NewsGroupDataset(hypo_data_indices_val, prem_data_indices_val, label_index_val,max_sentence_length)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, collate_fn=newsgroup_collate_func, shuffle=True)\n",
    "    #initialize model\n",
    "    model = LogisticRegressionPyTorch(len(id2token), emb_dim, len(set(label_index_val)), interaction_type)\n",
    "    best_model_save = '/Users/ludi/Desktop/tars/best_checkpoint5_log.tar'\n",
    "    checkpoint = torch.load(best_model_save)\n",
    "    # load the pretrained model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print('Validating on {} genre...'.format(genre))\n",
    "    val_acc, val_loss = test_model(val_loader, model, interaction_type)\n",
    "    mult_genre_val_acc[genre] = val_acc\n",
    "    print('The best logistic model\\'s validation accuracy on {} genre is {}.'.format(genre, np.around(val_acc,2)))\n",
    "\n",
    "pkl.dump(mult_genre_val_acc, open(\"3_2_Best_Logistic_Model_Evaluation_On_MNLI.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
